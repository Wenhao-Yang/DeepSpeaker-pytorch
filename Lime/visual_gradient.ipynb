{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@Author: yangwenhao\n",
    "@Contact: 874681044@qq.com\n",
    "@Software: PyCharm\n",
    "@File: input_compare.py\n",
    "@Time: 2020/3/25 5:30 PM\n",
    "@Overview:\n",
    "\"\"\"\n",
    "import argparse\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from python_speech_features import mel2hz\n",
    "from scipy import interpolate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = {}\n",
    "\n",
    "# extract_path='/home/yangwenhao/local/project/SpeakerVerification-pytorch/Data/gradient/LoResNet8/vox2/klsp_egs_baseline/arcsoft/Mean_cbam_None_dp01_alpha0_em256_var/epoch_50_var_50/epoch_50'\n",
    "cam_method = 'fullgrad'\n",
    "extract_path='/home/yangwenhao/local/project/SpeakerVerification-pytorch/Data/gradient/LoResNet8/vox2/klsp_egs_baseline/arcsoft/Mean_cbam_None_dp01_alpha0_em256_var/epoch_61_var_%s/epoch_61' % cam_method\n",
    "\n",
    "# Training options\n",
    "feat_dim = 161\n",
    "samples = 3000\n",
    "acoustic_feature = 'spectrogram' # choices=['fbank', 'spectrogram', 'mfcc'],\n",
    "seed = 123456\n",
    "load_from_pickle = False # load saved gradient and data \n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/work2020/yangwenhao/project/lstm_speaker_verification/data/vox2/dev/utt2spk'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-080c44ca1f96>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0muid2spk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/home/work2020/yangwenhao/project/lstm_speaker_verification/data/vox2/dev/utt2spk'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0muid\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msome_data\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/work2020/yangwenhao/project/lstm_speaker_verification/data/vox2/dev/utt2spk'"
     ]
    }
   ],
   "source": [
    "uid2spk = {}\n",
    "with open('/home/work2020/yangwenhao/project/lstm_speaker_verification/data/vox2/dev/utt2spk', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        uid, sid = l.split()\n",
    "        if uid in some_data:\n",
    "            uid2spk[uid] = sid\n",
    "    \n",
    "sid2gender = {}\n",
    "with open('/home/work2020/yangwenhao/project/lstm_speaker_verification/data/vox2/dev/spk2gender', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        sid, gender = l.split()\n",
    "        sid2gender[sid] = gender\n",
    "        \n",
    "uid2gender = {}\n",
    "for uid in uid2spk:\n",
    "    this_sid = uid2spk[uid]\n",
    "    \n",
    "    uid2gender[uid] = sid2gender[this_sid] if this_sid in sid2gender else 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets = ['orignal', 'babble', 'noise', 'music', 'reverb']\n",
    "\n",
    "# load selected input uids\n",
    "dir_path = pathlib.Path(extract_path)\n",
    "print('Path is %s' % str(dir_path))\n",
    "\n",
    "# inputs [train/valid/test]\n",
    "if load_from_pickle:\n",
    "    with open(extract_path + '/freq.data.pickle', 'rb') as f:\n",
    "        freq_data = pickle.load(f)  # avg on time axis\n",
    "    with open(extract_path + '/time.data.pickle', 'rb') as f:\n",
    "        time_data = pickle.load(f)  # avg on freq axis\n",
    "\n",
    "else:\n",
    "    train_lst = list(dir_path.glob('*train*bin'))\n",
    "    train_id_lst = list(dir_path.glob('*train*json'))\n",
    "    veri_lst = list(dir_path.glob('*ver*bin'))\n",
    "    valid_lst = list(dir_path.glob('*valid*bin'))\n",
    "    test_lst = list(dir_path.glob('*test*bin'))\n",
    "\n",
    "    print(' Train set extracting:')\n",
    "    time_data = []\n",
    "    num_utt = 0\n",
    "    for t in train_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'rb') as f:\n",
    "            sets = pickle.load(f)\n",
    "            for (data, grad) in sets:\n",
    "                time_data.append((data, grad))\n",
    "                num_utt += 1\n",
    "                if num_utt >= samples:\n",
    "                    break\n",
    "    \n",
    "    with open(extract_path + '/data_grad.pickle', 'wb') as f:\n",
    "        pickle.dump(time_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    time_data_uid = []\n",
    "    num_utt = 0\n",
    "    for t in train_id_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'r') as f:\n",
    "            sets = json.load(f)\n",
    "            for uid in sets:\n",
    "                time_data_uid.append(uid[0])\n",
    "                num_utt += 1\n",
    "                if num_utt >= samples:\n",
    "                    break\n",
    "                    \n",
    "    with open(extract_path + '/data_grad.uid.json', 'w') as f:\n",
    "        json.dump(time_data_uid, f)\n",
    "\n",
    "    freq_data = {}\n",
    "\n",
    "    train_data_mean = np.zeros((feat_dim))  # [data.mean/grad.abssum/grad.var]\n",
    "    train_time_mean = np.zeros((feat_dim))  # [data.mean/grad.abssum/grad.var]\n",
    "    train_time_var = np.zeros((feat_dim))\n",
    "    train_time_mean_part = np.zeros((feat_dim))\n",
    "\n",
    "    num_utt = 0\n",
    "    for t in train_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'rb') as f:\n",
    "            sets = pickle.load(f)\n",
    "            for (data, grad) in sets:\n",
    "#                 print(data.shape, \" \", grad.shape)\n",
    "#                 grad = np.array(grad.detach().squeeze())\n",
    "                if np.isnan(grad).sum() == 0:\n",
    "                    train_time_mean += np.mean(np.abs(grad), axis=0)\n",
    "                    train_time_var += np.var(grad, axis=0)\n",
    "                    train_data_mean += np.mean(data, axis=0)\n",
    "\n",
    "                    train_freq_sum = np.sum(grad, axis=1)\n",
    "                    train_grad_freq_sum_mean = train_freq_sum.mean()*0.8\n",
    "                    train_freq_grad = grad[np.where(train_freq_sum>train_grad_freq_sum_mean)[0]]\n",
    "                    train_time_mean_part += np.mean(np.abs(train_freq_grad), axis=0)\n",
    "\n",
    "                    num_utt += 1\n",
    "    \n",
    "    if num_utt>0:\n",
    "        print(\"The bumber of train utterance is %d.\" % num_utt)\n",
    "        train_time_mean /= num_utt\n",
    "        train_time_var /= num_utt\n",
    "        train_data_mean /= num_utt\n",
    "        train_time_mean_part /= num_utt\n",
    "\n",
    "    freq_data['train.time.mean'] = train_time_mean\n",
    "    freq_data['train.time.mean.part'] = train_time_mean_part\n",
    "    freq_data['train.time.var'] = train_time_var\n",
    "    freq_data['train.data.mean'] = train_data_mean\n",
    "\n",
    "    print(' Valid set extracting:')\n",
    "    valid_data_mean = np.zeros((feat_dim))  # [data.mean/grad.abssum/grad.var]\n",
    "    valid_time_mean = np.zeros((feat_dim))  # [data.mean/grad.abssum/grad.var]\n",
    "    valid_time_var = np.zeros((feat_dim))\n",
    "\n",
    "    valid_data = np.zeros((3, feat_dim))  # [data/grad]\n",
    "    num_utt = 0\n",
    "    for t in valid_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'rb') as f:\n",
    "            sets = pickle.load(f)\n",
    "            for (data, grad) in sets:\n",
    "#                 grad = np.array(grad.detach().squeeze())\n",
    "#                 valid_data_mean += np.mean(np.abs(data), axis=0)\n",
    "#                 valid_time_mean += np.mean(np.abs(grad), axis=0)\n",
    "                \n",
    "                valid_data_mean += np.mean(data, axis=0)\n",
    "                valid_time_mean += np.mean(np.abs(grad), axis=0)\n",
    "                \n",
    "                valid_time_var += np.var(grad, axis=0)\n",
    "\n",
    "                num_utt += 1\n",
    "    \n",
    "    if num_utt >0:\n",
    "        print(\"The bumber of valid utterance is %d.\" % num_utt)\n",
    "        valid_time_mean = valid_time_mean / num_utt\n",
    "        valid_time_var = valid_time_var / num_utt\n",
    "        valid_data_mean = valid_data_mean / num_utt\n",
    "\n",
    "    freq_data['valid.time.mean'] = valid_time_mean\n",
    "    freq_data['valid.time.var'] = valid_time_var\n",
    "    freq_data['valid.data.mean'] = valid_data_mean\n",
    "\n",
    "    print(' Train verification set extracting:')\n",
    "    veri_data = np.zeros((3, 2, feat_dim))  # [data/grad, utt_a, utt_b]\n",
    "\n",
    "    train_veri_data = np.zeros((feat_dim))\n",
    "    train_veri_mean = np.zeros((feat_dim))\n",
    "    train_veri_var = np.zeros((feat_dim))\n",
    "    train_veri_relu = np.zeros((feat_dim))\n",
    "\n",
    "    num_utt = 0\n",
    "    for t in veri_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'rb') as f:\n",
    "            sets = pickle.load(f)\n",
    "            for (label, grad_a, grad_b, data_a, data_b) in sets:\n",
    "                train_veri_data += (np.mean(data_a, axis=0) + np.mean(data_b, axis=0)) / 2\n",
    "                train_veri_mean += (np.mean(np.abs(grad_a), axis=0) + np.mean(np.abs(grad_b), axis=0)) / 2\n",
    "                train_veri_relu += (np.mean(np.where(grad_a > 0, grad_a, 0), axis=0) +\n",
    "                                    np.mean(np.where(grad_b > 0, grad_b, 0), axis=0)) / 2\n",
    "\n",
    "                train_veri_var += (np.var(grad_a, axis=0) + np.var(grad_b, axis=0)) / 2\n",
    "\n",
    "                num_utt += 1\n",
    "    \n",
    "    if num_utt > 0:\n",
    "        print(\"The bumber of valid utterance is %d.\" % num_utt)\n",
    "        train_veri_data /= num_utt\n",
    "        train_veri_mean /= num_utt\n",
    "        train_veri_var /= num_utt\n",
    "        train_veri_relu /= num_utt\n",
    "\n",
    "    freq_data['train.veri.time.mean'] = train_veri_mean\n",
    "    freq_data['train.veri.time.var'] = train_veri_var\n",
    "    freq_data['train.veri.data.mean'] = train_veri_data\n",
    "#     print(train_veri_data)\n",
    "    freq_data['train.veri.time.relu'] = train_veri_relu\n",
    "\n",
    "    print(' Test set extracting:')\n",
    "    # test_data = np.zeros((3, 2, args.feat_dim))  # [data/grad, utt_a, utt_b]\n",
    "    test_veri_data = np.zeros((feat_dim))\n",
    "    test_veri_mean = np.zeros((feat_dim))\n",
    "    test_veri_var = np.zeros((feat_dim))\n",
    "    test_veri_relu = np.zeros((feat_dim))\n",
    "\n",
    "    num_utt = 0\n",
    "    for t in test_lst:\n",
    "        p = str(t)\n",
    "        with open(p, 'rb') as f:\n",
    "            sets = pickle.load(f)\n",
    "            for (label, grad_a, grad_b, data_a, data_b) in sets:\n",
    "                test_veri_data += (np.mean(data_a, axis=0) + np.mean(data_b, axis=0)) / 2\n",
    "                test_veri_mean += (np.mean(np.abs(grad_a), axis=0) + np.mean(np.abs(grad_b), axis=0)) / 2\n",
    "                test_veri_relu += (np.mean(np.where(grad_a > 0, grad_a, 0), axis=0) + \n",
    "                                   np.mean(np.where(grad_b > 0, grad_b, 0), axis=0)) / 2\n",
    "\n",
    "                test_veri_var += (np.var(grad_a, axis=0) + np.var(grad_b, axis=0)) / 2\n",
    "\n",
    "                num_utt += 1\n",
    "    \n",
    "    if num_utt > 0:\n",
    "        test_veri_data /= num_utt\n",
    "        test_veri_mean /= num_utt\n",
    "        test_veri_var /= num_utt\n",
    "        test_veri_relu /= num_utt\n",
    "\n",
    "    freq_data['test.veri.time.mean'] = test_veri_mean\n",
    "    freq_data['test.veri.time.var'] = test_veri_var\n",
    "    freq_data['test.veri.data.mean'] = test_veri_data\n",
    "    freq_data['test.veri.time.relu'] = test_veri_relu\n",
    "\n",
    "    print('Saving inputs in %s' % extract_path)\n",
    "\n",
    "    with open(extract_path + '/freq.data.pickle', 'wb') as f:\n",
    "        pickle.dump(freq_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = {}\n",
    "for uid,(data, grad) in zip(time_data_uid, time_data):\n",
    "    some_data[uid]=[data, grad]\n",
    "    \n",
    "print(list(some_data.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_f = np.zeros((feat_dim))\n",
    "train_input_m = np.zeros((feat_dim))\n",
    "\n",
    "grad_input_f = np.zeros((feat_dim))\n",
    "grad_input_m = np.zeros((feat_dim))\n",
    "\n",
    "males = 0\n",
    "for uid in some_data:\n",
    "    gender = uid2gender[uid]\n",
    "    data,grad = some_data[uid]\n",
    "    if np.isnan(grad).sum()>0:\n",
    "        print(\"Skip %s with shape %s\" %(uid, str(grad.shape)))\n",
    "#         print(grad)\n",
    "        continue\n",
    "            \n",
    "    if gender =='f':\n",
    "            \n",
    "        train_input_f += data.mean(axis=0)\n",
    "        grad_input_f += np.abs(grad).mean(axis=0)\n",
    "    else:\n",
    "        males += 1\n",
    "        train_input_m += data.mean(axis=0)\n",
    "        grad_input_m += np.abs(grad).mean(axis=0)\n",
    "        \n",
    "females = len(some_data)-males\n",
    "print(\"The number of males and females are %d and %d.\" %(males, females))\n",
    "train_input_f /= females\n",
    "train_input_m /= males\n",
    "\n",
    "grad_input_f /= females\n",
    "grad_input_m /= males\n",
    "# print(grad_input_f, grad_input_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(train_input_f)\n",
    "ax.plot(train_input_m)\n",
    "ax.legend(['f', 'm'])\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(grad_input_f)\n",
    "ax.plot(grad_input_m)\n",
    "ax.legend(['f', 'm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cam_method in [\"grad_cam\", \"grad_cam_pp\", \"fullgrad\"]:\n",
    "    weight_path='/home/yangwenhao/local/project/SpeakerVerification-pytorch/Data/gradient/LoResNet8/vox2/klsp_egs_baseline/arcsoft/Mean_cbam_None_dp01_alpha0_em256_var/epoch_61_var_%s/epoch_61' % cam_method\n",
    "    with open(weight_path + '/freq.data.pickle', 'rb') as f:\n",
    "        freq_weight = pickle.load(f)\n",
    "    this_weight = freq_weight['train.time.mean']\n",
    "    plt.plot(this_weight/this_weight.sum())\n",
    "plt.legend([\"grad_cam\", \"grad_cam_pp\", \"fullgrad\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data [5, 2, 120, 161]\n",
    "# plotting filters distributions\n",
    "\n",
    "# train_data [numofutt, feats[N, 161]]\n",
    "train_input = freq_data['train.data.mean']\n",
    "valid_input = freq_data['valid.data.mean']\n",
    "train_grad = freq_data['train.time.mean']\n",
    "train_grad_part = freq_data['train.time.mean.part']\n",
    "print(train_grad)\n",
    "valid_grad = freq_data['valid.time.mean']\n",
    "\n",
    "x = np.arange(feat_dim) * 8000 / (feat_dim - 1)  # [0-8000]\n",
    "if acoustic_feature == 'fbank':\n",
    "    m = np.linspace(0, 2840.0230467083188, feat_dim)\n",
    "    x = mel2hz(m)\n",
    "\n",
    "# y = np.sum(all_data, axis=2)  # [5, 2, 162]\n",
    "plt.rc('font', family='Times New Roman')\n",
    "\n",
    "plt.figure(figsize=(16, 3))\n",
    "# plt.title('Gradient Distributions', fontsize=22)\n",
    "plt.title('All weight on frequency', fontsize=22)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=24)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.ylabel('Weight', fontsize=24)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "m = np.arange(0, 2840.0230467083188)\n",
    "m = 700 * (10 ** (m / 2595.0) - 1)\n",
    "n = np.array([m[i] - m[i - 1] for i in range(1, len(m))])\n",
    "n = 1 / n\n",
    "\n",
    "f = interpolate.interp1d(m[1:], n)\n",
    "xnew = np.arange(np.min(m[1:]), np.max(m[1:]), (np.max(m[1:]) - np.min(m[1:])) / 161)\n",
    "ynew = f(xnew)\n",
    "ynew = ynew / ynew.sum()#.max()\n",
    "# plt.plot(xnew, ynew)\n",
    "# print(np.sum(ynew))\n",
    "\n",
    "# for s in train_grad, valid_grad, veri_grad, veri_grad_relu, test_grad:\n",
    "# for s in veri_grad, veri_grad_relu:\n",
    "# for s in train_grad, veri_grad, test_grad:\n",
    "# for s in (np.abs(train_grad),train_grad_part) :\n",
    "for s in (train_grad_part, train_grad) :\n",
    "\n",
    "\n",
    "    # for s in test_a_set_grad, test_b_set_grad:\n",
    "    f = interpolate.interp1d(x, s)\n",
    "    xnew = np.linspace(np.min(x), np.max(x), 161)\n",
    "    ynew = f(xnew)\n",
    "    ynew = ynew / ynew.sum() #.max()\n",
    "    plt.plot(xnew, ynew, alpha=0.5)\n",
    "    # pdb.set_trace\n",
    "# if not os.path.exists(args.extract_path + '/grad.npy'):\n",
    "ynew = train_grad\n",
    "ynew = ynew / ynew.sum()\n",
    "\n",
    "# np.save(extract_path + '/train.grad.veri.npy', veri_grad)\n",
    "\n",
    "# plt.legend(['Mel-scale', 'Train', 'Valid', 'Test_a', 'Test_b'], loc='upper right', fontsize=18)\n",
    "# plt.legend(['Train', 'Valid', 'Train Verify', 'Train Verify Relu', 'Test'], loc='upper right', fontsize=24)\n",
    "plt.legend(['mel', 'input','Train', 'Train Verify', 'Test'], loc='upper right', fontsize=24)\n",
    "\n",
    "# plt.legend(['input', 'Train Verify', 'Train Verify Relu'], loc='upper right', fontsize=24)\n",
    "\n",
    "# plt.legend(['Mel-scale', 'Train', 'Valid', 'Train Verify', 'Test'], loc='upper right', fontsize=24)\n",
    "# pdf.savefig()\n",
    "# pdf.close()\n",
    "\n",
    "# plt.savefig(args.extract_path + \"/grads.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data [5, 2, 120, 161]\n",
    "# plotting filters distributions\n",
    "\n",
    "# train_data [numofutt, feats[N, 161]]\n",
    "train_input = freq_data['train.data.mean']\n",
    "valid_input = freq_data['valid.data.mean']\n",
    "test_input = freq_data['test.veri.data.mean']\n",
    "\n",
    "train_grad = freq_data['train.time.mean']\n",
    "train_grad_part = freq_data['train.time.mean.part']\n",
    "\n",
    "valid_grad = freq_data['valid.time.mean']\n",
    "veri_grad = freq_data['train.veri.time.mean']\n",
    "veri_grad_relu = freq_data['train.veri.time.relu']\n",
    "\n",
    "te3t_grad = freq_data['test.veri.time.mean']\n",
    "test_grad_relu = freq_data['test.veri.time.relu']\n",
    "\n",
    "x = np.arange(feat_dim) * 8000 / (feat_dim - 1)  # [0-8000]\n",
    "if acoustic_feature == 'fbank':\n",
    "    m = np.linspace(0, 2840.0230467083188, feat_dim)\n",
    "    x = mel2hz(m)\n",
    "\n",
    "# y = np.sum(all_data, axis=2)  # [5, 2, 162]\n",
    "plt.rc('font', family='Times New Roman')\n",
    "\n",
    "plt.figure(figsize=(16, 3))\n",
    "# plt.title('Gradient Distributions', fontsize=22)\n",
    "plt.title('All weight on frequency', fontsize=22)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=24)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.ylabel('Weight', fontsize=24)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "m = np.arange(0, 2840.0230467083188)\n",
    "m = 700 * (10 ** (m / 2595.0) - 1)\n",
    "n = np.array([m[i] - m[i - 1] for i in range(1, len(m))])\n",
    "n = 1 / n\n",
    "\n",
    "f = interpolate.interp1d(m[1:], n)\n",
    "xnew = np.arange(np.min(m[1:]), np.max(m[1:]), (np.max(m[1:]) - np.min(m[1:])) / 161)\n",
    "ynew = f(xnew)\n",
    "ynew = ynew / ynew.sum()#.max()\n",
    "plt.plot(xnew, ynew)\n",
    "# print(np.sum(ynew))\n",
    "\n",
    "# for s in train_grad, valid_grad, veri_grad, veri_grad_relu, test_grad:\n",
    "# for s in veri_grad, veri_grad_relu:\n",
    "# for s in train_grad, veri_grad, test_grad:\n",
    "# for s in (np.abs(train_grad),train_grad_part) :\n",
    "for s in (train_grad_part, veri_grad) :\n",
    "\n",
    "\n",
    "    # for s in test_a_set_grad, test_b_set_grad:\n",
    "    f = interpolate.interp1d(x, s)\n",
    "    xnew = np.linspace(np.min(x), np.max(x), 161)\n",
    "    ynew = f(xnew)\n",
    "    ynew = ynew / ynew.sum() #.max()\n",
    "    plt.plot(xnew, ynew, alpha=0.5)\n",
    "    # pdb.set_trace\n",
    "# if not os.path.exists(args.extract_path + '/grad.npy'):\n",
    "ynew = veri_grad\n",
    "ynew = ynew / ynew.sum()\n",
    "\n",
    "np.save(extract_path + '/train.grad.veri.npy', veri_grad)\n",
    "\n",
    "# plt.legend(['Mel-scale', 'Train', 'Valid', 'Test_a', 'Test_b'], loc='upper right', fontsize=18)\n",
    "# plt.legend(['Train', 'Valid', 'Train Verify', 'Train Verify Relu', 'Test'], loc='upper right', fontsize=24)\n",
    "plt.legend(['input','Train', 'Train Verify', 'Test'], loc='upper right', fontsize=24)\n",
    "\n",
    "# plt.legend(['input', 'Train Verify', 'Train Verify Relu'], loc='upper right', fontsize=24)\n",
    "\n",
    "# plt.legend(['Mel-scale', 'Train', 'Valid', 'Train Verify', 'Test'], loc='upper right', fontsize=24)\n",
    "# pdf.savefig()\n",
    "# pdf.close()\n",
    "\n",
    "# plt.savefig(args.extract_path + \"/grads.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Data distributions', fontsize=22)\n",
    "plt.xlabel('Frequency (Hz)', fontsize=16)\n",
    "plt.ylabel('Log Power (-)', fontsize=16)\n",
    "# 插值平滑 ？？？\n",
    "for s in train_input, valid_input, test_input:\n",
    "    # for s in test_a_set_grad, test_b_set_grad:\n",
    "    f = interpolate.interp1d(x, s)\n",
    "    xnew = np.linspace(np.min(x), np.max(x), 161)\n",
    "    ynew = f(xnew)\n",
    "    plt.plot(xnew, ynew)\n",
    "\n",
    "plt.legend(['Train', 'Valid', 'Test'], loc='upper right', fontsize=16)\n",
    "# plt.savefig(args.extract_path + \"/inputs.freq.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Data distributions in Time Axis', fontsize=22)\n",
    "plt.xlabel('Time', fontsize=16)\n",
    "plt.ylabel('Magnetitude', fontsize=16)\n",
    "# 插值平滑 ？？？\n",
    "# for i, (data, grad) in enumerate(time_data):\n",
    "# for s in test_a_set_grad, test_b_set_grad:\n",
    "data = time_data[0][0]\n",
    "grad = time_data[0][1]\n",
    "norm = matplotlib.colors.Normalize(vmin=0., vmax=1.)\n",
    "# data_mean = data.mean(axis=10\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "\n",
    "# data = (data - data.min()) / (data.max() - data.min())\n",
    "# im = ax.imshow(np.log(data.transpose()), cmap='viridis', aspect='auto')\n",
    "im = ax.imshow(data.transpose(), cmap='viridis', aspect='auto')\n",
    "# print(data.min(), data.max())\n",
    "plt.colorbar(im)  # 显示颜色标尺\n",
    "# ax.plot(data_mean)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "grad = np.abs(grad)\n",
    "grad_mean = grad\n",
    "# grad_mean = (grad - grad.min()) / (grad.max() - grad.min())\n",
    "# im = ax.imshow(1/np.log(grad_mean.transpose()), norm=norm, cmap='viridis', aspect='auto')\n",
    "im = ax.imshow(grad_mean.transpose(), cmap='viridis', aspect='auto')\n",
    "# ax.plot(np.log(grad_mean))\n",
    "ax.set_xlim(0, len(grad_mean))\n",
    "\n",
    "# plt.legend(['Train', 'Valid', 'Test'], loc='upper right', fontsize=16)\n",
    "plt.colorbar(im)  # 显示颜色标尺\n",
    "# plt.savefig(args.extract_path + \"/inputs.time.png\")\n",
    "plt.show()\n",
    "\n",
    "print('Completed!\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}