{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "@Author: yangwenhao\n",
    "@Contact: 874681044@qq.com\n",
    "@Software: PyCharm\n",
    "@File: cam_2.py\n",
    "@Time: 2021/4/12 21:47\n",
    "@Overview:\n",
    "    Created on 2019/8/4 上午9:37\n",
    "    @author: mick.yi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel\n",
    "\n",
    "from Define_Model.ResNet import ThinResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "torch.distributed.init_process_group(backend=\"nccl\", init_method='tcp://localhost:12556', rank=0,\n",
    "                                     world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(object):\n",
    "    \"\"\"\n",
    "    1: 网络不更新梯度,输入需要梯度更新\n",
    "    2: 使用目标类别的得分做反向传播\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, layer_name):\n",
    "        self.net = net\n",
    "        self.layer_name = layer_name\n",
    "        self.feature = {}\n",
    "        self.gradient = {}\n",
    "        self.net.eval()\n",
    "        self.handlers = []\n",
    "        self._register_hook()\n",
    "\n",
    "    def _get_features_hook(self, module, input, output):\n",
    "        print(type(module))\n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            self.feature[input[0].device] = output[0]\n",
    "        else:\n",
    "            self.feature = output[0]\n",
    "#         print(\"Device {}, forward out feature shape:{}\".format(input[0].device, output[0].size()))\n",
    "\n",
    "    def _get_grads_hook(self, module, input_grad, output_grad):\n",
    "        \"\"\"\n",
    "        :param input_grad: tuple, input_grad[0]: None\n",
    "                                   input_grad[1]: weight\n",
    "                                   input_grad[2]: bias\n",
    "        :param output_grad:tuple,长度为1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            if input_grad[0].device not in self.gradient:\n",
    "                self.gradient[input_grad[0].device] = output_grad[0]\n",
    "            else:\n",
    "                self.gradient[input_grad[0].device] += output_grad[0]\n",
    "        else:\n",
    "            self.gradient += output_grad[0]\n",
    "        \n",
    "#         print(output_grad[0])\n",
    "#         print(\"Device {}, backward out gradient shape:{}\".format(input_grad[0].device, output_grad[0].size()))\n",
    "\n",
    "    def _register_hook(self):\n",
    "\n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            modules = self.net.module.named_modules()\n",
    "        else:\n",
    "            modules = self.net.named_modules()\n",
    "\n",
    "        for (name, module) in modules:\n",
    "            if name == self.layer_name:\n",
    "                self.handlers.append(module.register_backward_hook(self._get_features_hook))\n",
    "                self.handlers.append(module.register_backward_hook(self._get_grads_hook))\n",
    "\n",
    "    def remove_handlers(self):\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, inputs, index):\n",
    "        \"\"\"\n",
    "        :param inputs: [1,3,H,W]\n",
    "        :param index: class id\n",
    "        :return:\n",
    "        \"\"\"\n",
    "#         self.net.zero_grad()\n",
    "\n",
    "        output, _ = self.net(inputs)  # [1,num_classes]\n",
    "        pdb.set_trace()\n",
    "\n",
    "        if index is None:\n",
    "            index = torch.argmax(output)\n",
    "            \n",
    "        target = output.gather(1, index)# .mean()\n",
    "\n",
    "        # target = output[0][index]\n",
    "        for i in target:\n",
    "            i.backward(retain_graph=True)\n",
    "        \n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            feature = []\n",
    "            gradient = []\n",
    "            for d in self.gradient:\n",
    "                feature.append(self.feature[d])\n",
    "                gradient.append(self.gradient[d])\n",
    "\n",
    "            feature = torch.cat(feature, dim=0)\n",
    "            gradient = torch.cat(gradient, dim=0)\n",
    "        else:\n",
    "            feature = self.feature\n",
    "            gradient = self.gradient\n",
    "            \n",
    "        return feature, gradient\n",
    "        \n",
    "        # gradient = self.gradient[0].cpu().data.numpy()  # [C,H,W]\n",
    "        # weight = np.mean(gradient, axis=(1, 2))  # [C]\n",
    "        # feature = self.feature[0].cpu().data.numpy()  # [C,H,W]\n",
    "\n",
    "        # cam = feature * weight[:, np.newaxis, np.newaxis]  # [C,H,W]\n",
    "        # cam = np.sum(cam, axis=0)  # [H,W]\n",
    "        # cam = np.maximum(cam, 0)  # ReLU\n",
    "        #\n",
    "        # # 数值归一化\n",
    "        # cam -= np.min(cam)\n",
    "        # cam /= np.max(cam)\n",
    "        # # resize to 224*224\n",
    "        # cam = cv2.resize(cam, (224, 224))\n",
    "        # return cam\n",
    "        \n",
    "#         print(\"gradient shape: \", gradient.shape)\n",
    "#         print(\"feature shape: \", feature.shape)\n",
    "\n",
    "class Sum_GradCAM(object):\n",
    "    \"\"\"\n",
    "    1: 网络不更新梯度,输入需要梯度更新\n",
    "    2: 使用目标类别的得分做反向传播\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net, layer_name):\n",
    "        self.net = net\n",
    "        self.layer_name = layer_name\n",
    "        self.feature = {}\n",
    "        self.gradient = {}\n",
    "        self.net.eval()\n",
    "        self.handlers = []\n",
    "        self._register_hook()\n",
    "\n",
    "    def _get_features_hook(self, module, input, output):\n",
    "        \n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            self.feature[input[0].device] = output[0]\n",
    "        else:\n",
    "            self.feature = output[0]\n",
    "#         print(\"Device {}, forward out feature shape:{}\".format(input[0].device, output[0].size()))\n",
    "\n",
    "    def _get_grads_hook(self, module, input_grad, output_grad):\n",
    "        \"\"\"\n",
    "        :param input_grad: tuple, input_grad[0]: None\n",
    "                                   input_grad[1]: weight\n",
    "                                   input_grad[2]: bias\n",
    "        :param output_grad:tuple,长度为1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            if input_grad[0].device not in self.gradient:\n",
    "                self.gradient[input_grad[0].device] = output_grad[0]\n",
    "            else:\n",
    "                self.gradient[input_grad[0].device] += output_grad[0]\n",
    "        else:\n",
    "            self.gradient = output_grad[0]\n",
    "        \n",
    "#         print(output_grad[0])\n",
    "#         print(\"Device {}, backward out gradient shape:{}\".format(input_grad[0].device, output_grad[0].size()))\n",
    "\n",
    "    def _register_hook(self):\n",
    "\n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            modules = self.net.module.named_modules()\n",
    "        else:\n",
    "            modules = self.net.named_modules()\n",
    "\n",
    "        for (name, module) in modules:\n",
    "            if name == self.layer_name:\n",
    "                self.handlers.append(module.register_backward_hook(self._get_features_hook))\n",
    "                self.handlers.append(module.register_backward_hook(self._get_grads_hook))\n",
    "\n",
    "    def remove_handlers(self):\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "    def __call__(self, inputs, index):\n",
    "        \"\"\"\n",
    "        :param inputs: [1,3,H,W]\n",
    "        :param index: class id\n",
    "        :return:\n",
    "        \"\"\"\n",
    "#         self.net.zero_grad()\n",
    "\n",
    "        output, _ = self.net(inputs)  # [1,num_classes]\n",
    "        pdb.set_trace()\n",
    "\n",
    "        if index is None:\n",
    "            index = torch.argmax(output)\n",
    "            \n",
    "        target = output.gather(1, index).mean()\n",
    "        target.backward(retain_graph=True)\n",
    "        \n",
    "        if isinstance(self.net, DistributedDataParallel):\n",
    "            feature = []\n",
    "            gradient = []\n",
    "            for d in self.gradient:\n",
    "                feature.append(self.feature[d])\n",
    "                gradient.append(self.gradient[d])\n",
    "\n",
    "            feature = torch.cat(feature, dim=0)\n",
    "            gradient = torch.cat(gradient, dim=0)\n",
    "        else:\n",
    "            feature = self.feature\n",
    "            gradient = self.gradient\n",
    "        \n",
    "        return feature, gradient\n",
    "        \n",
    "#         print(\"gradient shape: \", gradient.shape)\n",
    "#         print(\"feature shape: \", feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/local/project/SpeakerVerification-pytorch/Define_Model/ResNet.py:374: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(m.weight, mean=0., std=1.)\n",
      "/home/yangwenhao/local/project/SpeakerVerification-pytorch/Define_Model/ResNet.py:376: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.weight, 1)\n",
      "/home/yangwenhao/local/project/SpeakerVerification-pytorch/Define_Model/ResNet.py:377: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "model = ThinResNet()\n",
    "model = model.cuda()\n",
    "model = DistributedDataParallel(model)\n",
    "gc = GradCAM(model, 'layer4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-3-194ac11b6417>(70)__call__()\n",
      "-> if index is None:\n",
      "(Pdb) c\n",
      "<class 'torch.nn.modules.container.Sequential'><class 'torch.nn.modules.container.Sequential'>\n",
      "\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((20, 1, 224, 224)).cuda() # *1.2 +1.\n",
    "l = torch.range(0, 19).long().unsqueeze(1).cuda()\n",
    "y = model(x)\n",
    "\n",
    "#\n",
    "cam = gc(x, l)\n",
    "# print(cam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
