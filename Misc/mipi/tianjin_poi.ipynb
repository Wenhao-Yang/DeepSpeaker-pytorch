{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "60bee833-3284-4cd5-8660-a84128879486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shapefile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xlrd3\n",
    "import os\n",
    "import glob\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd169673-d272-42d8-9809-22aad949baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:38<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# read poi from shp files\n",
    "if os.path.exists('tianjin.csv'):\n",
    "    names = pd.read_csv('tianjin.csv').name\n",
    "else:\n",
    "    shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "    places_names = []\n",
    "    for f in tqdm(shp_files, ncols=50):\n",
    "        x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "        places_names.append(x.name)\n",
    "    names = pd.concat(places_names, axis=0)\n",
    "    names.to_csv('tianjin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7dfcda49-97ee-40a5-8de3-39f32895c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenbelt = 'greenbelt'\n",
    "greenbelt_list = pd.read_excel('/home/yangwenhao/local/project/weibo_filter/app/data/' + greenbelt + '.xls')\n",
    "green_loc = set(greenbelt_list.name.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fb1b5457-e207-4ea2-a419-84c5cd7e1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 of green park (124) locations are not appeared in full list\n",
      "[\"Tianjin Bohai Children's World\", '南湖北海公园', '西苑住宿楼公园（南）', '龙园（在建）', '滨河公园', '生态谷公园（八区）', '西苑住宿楼公园（北）', '学府园 Xuefu Park', '贝壳堤湿地公园', '运动公园', '万新公园', 'The Sports Park', '幸福公园', 'sport', '生育文化园', '临时绿地', 'Royal Park', '三盘暮雨', '生态谷公园（三区）', '北宁公园', '大港公园', '社区健身园', '海河外滩公园', 'Garten um die süd strasse', '天津文化中心', '东疆湾建设纪念公园', '生态谷公园（六区）', '绿地及地下停车场', '绿道公园', '中新友好公园', '滨海文化中心', '苍峰园', '李纯祠堂', 'Tailay Park', '龙湾儿童游乐园', '生态谷公园盐田文化风貌区（四区）', '绿屏双桥河湾公园', '河西公园', '生态谷公园（五区）', '抗日英雄纪念碑', '月牙河公园', '津南小站钻石公园', '生态谷公园（二区）', '生态谷公园雕塑文化教育区（七区）', '集贤公园', '人民广场', '朝霞公园']\n"
     ]
    }
   ],
   "source": [
    "only_loc = []\n",
    "plain_loc = set(names.to_list())\n",
    "for g in green_loc:\n",
    "    if g not in plain_loc:\n",
    "        only_loc.append(g)\n",
    "\n",
    "print(\"%d of green park (%d) locations are not appeared in full list\" % (len(only_loc), len(green_loc)))\n",
    "print(only_loc)\n",
    "unique_green_loc = pd.DataFrame(only_loc, columns=['name'])\n",
    "if not os.path.exists('unique_green.csv'):\n",
    "    unique_green_loc.to_csv('unique_green.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "02dcf649-4376-4cef-826c-11cc9d860a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tianjin Bohai Children's World', '南湖北海公园', '西苑住宿楼公园（南）', '龙园（在建）', '生态谷公园（八区）', '西苑住宿楼公园（北）', '学府园 Xuefu Park', '万新公园', 'The Sports Park', '生育文化园', '临时绿地', 'Royal Park', '生态谷公园（三区）', 'Garten um die süd strasse', '东疆湾建设纪念公园', '生态谷公园（六区）', '绿地及地下停车场', 'Tailay Park', '龙湾儿童游乐园', '生态谷公园盐田文化风貌区（四区）', '绿屏双桥河湾公园', '生态谷公园（五区）', '抗日英雄纪念碑', '津南小站钻石公园', '生态谷公园（二区）', '生态谷公园雕塑文化教育区（七区）', '朝霞公园', "
     ]
    }
   ],
   "source": [
    "for l in only_loc:\n",
    "    if len(names[names.str.contains(l)]) == 0:\n",
    "        print(\"\\'%s\\'\"%l, end=', ')\n",
    "# Tianjin Bohai Children's World  ==  天津宋庆龄渤海儿童世界\n",
    "# 生态谷公园（六区）== '生态谷公园雕刻文化教育区(6区)'\n",
    "# 生态谷公园盐田文化风貌区（四区） = 生态谷公园盐田文化风貌区(4区)\n",
    "# 生态谷公园（五区）= 生态谷公园雕塑文化教育区(5区)\n",
    "# 津南小站钻石公园 == '津南·小站钻石公园停车点', '津南·小站钻石公园'\n",
    "# 生态谷公园雕塑文化教育区（七区）== 生态谷公园雕刻文化教育区(7区)\n",
    "# 绿屏双桥河湾公园"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "96137d5c-9ee4-4aa3-b820-bdbbeed066ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['派多格猫咪(北宁公园店)',\n",
       " '一东(北宁公园店)',\n",
       " '泰色足道养生馆(北宁公园店)',\n",
       " '来电(格林豪泰酒店北宁公园店)',\n",
       " '犟骨头排骨饭(北宁公园店)',\n",
       " '芳竹园火锅(北宁公园店)',\n",
       " '百草蒸汽海鲜(北宁公园店)',\n",
       " '北宁公园(公交站)',\n",
       " '北宁公园(地铁站)',\n",
       " '北宁公园地铁站(公交站)',\n",
       " '格林豪泰酒店(北宁公园店)停车场',\n",
       " '瑞步青少年篮球培训(北宁公园校区)',\n",
       " '99优选酒店(天津北站北宁公园地铁站店)',\n",
       " '如家睿柏·云五星级酒店(天津中山北路北宁公园地铁站店)',\n",
       " '格林豪泰酒店(北宁公园店)',\n",
       " '汇充汽车充电站(北宁公园)',\n",
       " '北宁公园充电站',\n",
       " '北宁公园充电站']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[names.str.contains('北宁公园')].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b037eb-73aa-4b8c-8935-6e884ec2fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read poi and locations from shp files\n",
    "shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "places_names_loc = []\n",
    "for f in tqdm(shp_files, ncols=100):\n",
    "    # print(f)\n",
    "    x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "    places_names_loc.append(x[['name', 'geometry']])\n",
    "    \n",
    "places_names_loc = pd.concat(places_names_loc)\n",
    "names2loc = {}\n",
    "for l in places_names_loc.iterrows():\n",
    "    # print(l[1].values[0], l[1].values[1],)\n",
    "    names2loc[l[1].values[0]] = l[1].values[1]\n",
    "\n",
    "    \n",
    "with open('names2loc.pickle', 'wb') as f:\n",
    "    pickle.dump(names2loc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20ece0f1-b6cd-44e6-aaa8-27ce8ef76935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    元记电气焊补胎钣金\n",
       "1       元汇津通卸载\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d14c45-9cf5-4540-90b2-f4f14fb427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_names = greenbelt_list.name # names\n",
    "location_names = names\n",
    "numofprint = 4\n",
    "\n",
    "# 'first_2w'\n",
    "# for lst in ['first_2w', 'noimg_6w']:\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    \n",
    "    text_csv = 'all_list/' + '%s.csv'%(lst)\n",
    "    if not os.path.exists(text_csv):\n",
    "        data = xlrd3.open_workbook('/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx')\n",
    "        table = data.sheet_by_index(0)#.sheets()[0]          #通过索引顺序获取\n",
    "\n",
    "        # get full-text tweets\n",
    "        texts = []\n",
    "        if lst == 'first_2w':\n",
    "            test_idx = 5\n",
    "        elif lst == 'list2_img':\n",
    "            test_idx = 4\n",
    "        elif lst == 'list2_woimg':\n",
    "            test_idx = 6\n",
    "        else:\n",
    "            test_idx = 6\n",
    "\n",
    "        for i in range(1, table.nrows):\n",
    "            texts.append(table.row(i)[test_idx].value)\n",
    "\n",
    "        texts = pd.DataFrame(texts, columns=['text'])\n",
    "        texts.to_csv('%s.csv'%(lst), index=False)\n",
    "    else:\n",
    "        texts = pd.read_csv(text_csv)\n",
    "    \n",
    "    # for i, texts in enumerate(pd.read_csv(text_csv, chunksize=50000)):\n",
    "    \n",
    "    # match all palces for all lines\n",
    "    places,sparse_matirx = [],[]\n",
    "    for idx, i in tqdm(enumerate(location_names.unique())): #[297604:]\n",
    "        try:\n",
    "            result = texts.text.str.contains(i) # 12:26:26\n",
    "            # result = texts.apply(apply_method, axis=0, location=i) # 17:11:27\n",
    "            # result = texts.parallel_apply(apply_method, axis=0, location=i) # 76:42:44\n",
    "            if result.to_numpy().sum() > 0:\n",
    "                places.append(i)\n",
    "                sparse_matirx.append(result.to_numpy())\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        if (idx+1) % 200000 == 0 or (idx+1) == len(location_names.unique()):\n",
    "            print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "            with open('all_list/place2index_%s_%d.pickle' % (lst,idx), 'wb') as f:\n",
    "                pickle.dump([places, sparse_matirx], f)\n",
    "            places,sparse_matirx = [],[]\n",
    "\n",
    "    # save match results with pickle\n",
    "#     print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "#     # with open('all_list/place2index_%s_%d.pickle' % (lst,i), 'wb') as f:\n",
    "#     #     pickle.dump([places, sparse_matirx], f)\n",
    "#     with open('all_list/place2index_%s.pickle' % (lst), 'wb') as f:\n",
    "#         pickle.dump([places, sparse_matirx], f)\n",
    "        \n",
    "#     # print the most places in all tweets\n",
    "#     name2num = []\n",
    "#     for i,p in enumerate(places):\n",
    "#         name2num.append(sparse_matirx[i].sum())\n",
    "\n",
    "#     name2num = np.array(name2num)\n",
    "#     order_num = np.flip(np.argsort(name2num))\n",
    "#     np_places = np.array(places)\n",
    "#     for p, n in zip(np_places[order_num][:numofprint], name2num[order_num][:numofprint]):\n",
    "#         print(\"{:14<}\".format(p), \":\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a39bd5c6-6575-40e7-9807-f3d491ab6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 14/14 [00:15<00:00,  1.08s/it]\n",
      " 31%|████████████████▌                                    | 90783/289992 [00:07<00:15, 12543.94it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "row index was 65536, not allowed by .xls format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_384289/3682540257.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mnpoi_row\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mwritesheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappend_cells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwritesheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/xlwt/Worksheet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, r, c, label, style)\u001b[0m\n\u001b[1;32m   1086\u001b[0m            \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mxlwt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXFStyle\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \"\"\"\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_rich_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrich_text_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/xlwt/Worksheet.py\u001b[0m in \u001b[0;36mrow\u001b[0;34m(self, indx)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flushed_rows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempt to reuse row index %d of sheet %r after flushing\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_used_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_used_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/xlwt/Row.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rowx, parent_sheet)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_sheet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrowx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m65535\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"row index was %r, not allowed by .xls format\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrowx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrowx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_sheet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: row index was 65536, not allowed by .xls format"
     ]
    }
   ],
   "source": [
    "# save match results with pickle\n",
    "# for lst in ['list2_img', 'list2_woimg']:\n",
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "names2loc_key = set(list(names2loc.keys()))\n",
    "\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    append_cells = {}\n",
    "    text_csv = 'all_list/' + '%s_*.csv'%(lst)\n",
    "    text_csvs = glob.glob(text_csv)\n",
    "\n",
    "    for t in tqdm(text_csvs, ncols=100):\n",
    "        this_texts = pd.read_csv(t, index_col=0)\n",
    "        with open(t.replace('csv', 'pickle'), 'rb') as f:\n",
    "            [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "        places_np = np.array(places)\n",
    "        sparse_np = np.array(sparse_matirx) #.shape\n",
    "        \n",
    "        if places_np.shape[0] > 0:\n",
    "            for j, l in zip(this_texts.index, range(sparse_np.shape[1])):\n",
    "                row_cells = []\n",
    "                for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "                    if i in names2loc_key:\n",
    "                        i_xy = names2loc[i]\n",
    "                        row_cells.append([i, i_xy.x, i_xy.y])\n",
    "                    else:\n",
    "                        row_cells.append([i, \"\", \"\"])\n",
    "\n",
    "                append_cells[j] = row_cells\n",
    "                \n",
    "#     with open('all_list/' + '%s_green.pickle'%(lst), 'rb') as f:\n",
    "#         [places, sparse_matirx] = pickle.load(f)\n",
    "#         places_np = np.array(places)\n",
    "#         sparse_np = np.array(sparse_matirx) #.shape\n",
    "        \n",
    "#         if places_np.shape[0] > 0:\n",
    "#             for l in range(sparse_np.shape[1]):\n",
    "#                 row_cells = []\n",
    "#                 for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "#                     i_xy = names2loc[i]\n",
    "#                     row_cells.append([i, i_xy.x, i_xy.y])\n",
    "                    \n",
    "#                 append_cells[l+1].extend(row_cells)  \n",
    "    if lst == 'list2_img':\n",
    "        read_cols = [2, 3, 4, 5, 6, 7, 8,  9]\n",
    "    elif lst == 'list2_woimg':\n",
    "        read_cols = [2, 3, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    read_writebook = xlrd3.open_workbook(filename=read_lst)\n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "            \n",
    "    # write header\n",
    "    # find the tweet with the largest number of locations\n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(append_cells[i]))\n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "        \n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    nrows = read_table.nrows\n",
    "    \n",
    "    assert (nrows-1) == len(append_cells), print(\"rows in excel: \", (nrows-1), \"rows in loc_csv: \", len(append_cells))\n",
    "    for i in tqdm(range(1, nrows), ncols=100):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "        \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))     \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers),   label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)\n",
    "    print(save_lst, \" completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "67bc8cde-61fc-46a4-8bd1-f1b0e9494f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 14/14 [00:14<00:00,  1.00s/it]\n",
      "100%|████████████████████████████████████████████████████| 289992/289992 [00:18<00:00, 16080.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/local/project/weibo_filter/app/data/list2_img_loc.xlsx  completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 14/14 [00:06<00:00,  2.17it/s]\n",
      "100%|████████████████████████████████████████████████████| 176968/176968 [00:12<00:00, 14210.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/local/project/weibo_filter/app/data/list2_woimg_loc.xlsx  completed!\n"
     ]
    }
   ],
   "source": [
    "# save match results with pickle\n",
    "# for lst in ['list2_img', 'list2_woimg']:\n",
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "names2loc_key = set(list(names2loc.keys()))\n",
    "\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    append_cells = {}\n",
    "    text_csv = 'all_list/' + '%s_*.csv'%(lst)\n",
    "    text_csvs = glob.glob(text_csv)\n",
    "\n",
    "    for t in tqdm(text_csvs, ncols=100):\n",
    "        this_texts = pd.read_csv(t, index_col=0)\n",
    "        with open(t.replace('csv', 'pickle'), 'rb') as f:\n",
    "            [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "        places_np = np.array(places)\n",
    "        sparse_np = np.array(sparse_matirx) #.shape\n",
    "        \n",
    "        if places_np.shape[0] > 0:\n",
    "            for j, l in zip(this_texts.index, range(sparse_np.shape[1])):\n",
    "                row_cells = []\n",
    "                for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "                    if i in names2loc_key:\n",
    "                        i_xy = names2loc[i]\n",
    "                        row_cells.append([i, i_xy.x, i_xy.y])\n",
    "                    else:\n",
    "                        row_cells.append([i, \"\", \"\"])\n",
    "\n",
    "                append_cells[j] = row_cells\n",
    "                \n",
    "#     with open('all_list/' + '%s_green.pickle'%(lst), 'rb') as f:\n",
    "#         [places, sparse_matirx] = pickle.load(f)\n",
    "#         places_np = np.array(places)\n",
    "#         sparse_np = np.array(sparse_matirx) #.shape\n",
    "        \n",
    "#         if places_np.shape[0] > 0:\n",
    "#             for l in range(sparse_np.shape[1]):\n",
    "#                 row_cells = []\n",
    "#                 for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "#                     i_xy = names2loc[i]\n",
    "#                     row_cells.append([i, i_xy.x, i_xy.y])\n",
    "                    \n",
    "#                 append_cells[l+1].extend(row_cells)  \n",
    "    if lst == 'list2_img':\n",
    "        read_cols = [2, 3, 4, 5, 6, 7, 8,  9]\n",
    "    elif lst == 'list2_woimg':\n",
    "        read_cols = [2, 3, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xlsx'\n",
    "\n",
    "    read_writebook = xlrd3.open_workbook(filename=read_lst)\n",
    "    writebook = openpyxl.Workbook()\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.create_sheet(title=u'地点检索到公园POI',   index=0)\n",
    "    npoi_sheet    = writebook.create_sheet(title=u'地点检索到非公园POI', index=1)\n",
    "    tianjin_sheet = writebook.create_sheet(title=u'地点仅检索到天津',    index=2)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 2, 2, 2 # openpyxl start from 1 instead of 0\n",
    "    # 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.cell(1, i+1, h)\n",
    "            \n",
    "    # write header\n",
    "    # find the tweet with the largest number of locations\n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(append_cells[i]))\n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.cell(1, (i-1)*3+len(headers)+1, '地点提取%d'%i)\n",
    "            sheet.cell(1, (i-1)*3+len(headers)+2, '经度%d'%i)\n",
    "            sheet.cell(1, (i-1)*3+len(headers)+3, '纬度%d'%i)\n",
    "            \n",
    "    tianjin_sheet.cell(1, len(headers), '地点提取')\n",
    "    \n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    nrows = read_table.nrows\n",
    "    \n",
    "    assert (nrows-1) == len(append_cells), print(\"rows in excel: \", (nrows-1), \"rows in loc_csv: \", len(append_cells))\n",
    "    for i in tqdm(range(1, nrows), ncols=100):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "        \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.cell(this_row, 1, len(append_cells[i-1]))     \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.cell(this_row, j+2, this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.cell(this_row, j*3+len(headers)+1,   name)\n",
    "            writesheet.cell(this_row, j*3+len(headers)+2,   x)\n",
    "            writesheet.cell(this_row, j*3+len(headers)+3,   y)\n",
    "\n",
    "    writebook.save(save_lst)\n",
    "    print(save_lst, \" completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f540784-5005-441a-af33-8604f25466b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176968"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(append_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff95bc1-1bc7-4d02-b920-b9ef135a405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_texts = pd.read_csv('all_list/list2_img_1.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae5f894-64c1-449e-812f-f59559feef28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([20714, 20715, 20716, 20717, 20718, 20719, 20720, 20721, 20722,\n",
       "            20723,\n",
       "            ...\n",
       "            41418, 41419, 41420, 41421, 41422, 41423, 41424, 41425, 41426,\n",
       "            41427],\n",
       "           dtype='int64', length=20714)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_texts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0603a-1645-42b9-87e4-240c1fb54418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchProcess(tweet_csv, loc_csv, pickle_path):\n",
    "    texts = pd.read_csv(tweet_csv).text.fillna('')\n",
    "    location_names = pd.read_csv(loc_csv).name\n",
    "    \n",
    "    places,sparse_matirx = [],[]\n",
    "    \n",
    "    # for i in tqdm(['天津'], ncols=100): #[297604:]\n",
    "    for i in tqdm(location_names.unique(), ncols=100): #[297604:]\n",
    "        try:\n",
    "            result = texts.str.contains(i)\n",
    "            if result.to_numpy().sum() > 0:\n",
    "                places.append(i)\n",
    "                sparse_matirx.append(result.to_numpy())\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            continue\n",
    "        \n",
    "    print(tweet_csv, \"\\n 出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump([places, sparse_matirx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed21e6-285a-4224-a52e-29002dab6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with list2_woimg_5.csv\n",
    "MatchProcess('all_list/list2_woimg_5.csv', 'tianjin.csv', 'all_list/list2_woimg_5.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "577f4ef8-1219-42cc-ac8c-67389559d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_list/list2_img.csv \n",
      " 出现的地点数:  15  记录的列数: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# deal with green location\n",
    "MatchProcess('all_list/list2_woimg.csv', \n",
    "             'unique_green.csv',\n",
    "             'all_list/list2_woimg_green.pickle')\n",
    "\n",
    "MatchProcess('all_list/list2_img.csv', \n",
    "             'unique_green.csv',\n",
    "             'all_list/list2_img_green.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db4d629c-a41c-47f5-9024-72ecd4bd34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.load('test.npy', allow_pickle=True)\n",
    "texts = pd.read_csv('all_list/list2_woimg_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a2296b7-8ed3-4da5-aa7d-c81cc4fd4aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.fillna('').text[2854]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8f0546a-6c20-4907-92db-4bbeed2fb295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lines:  289992 Sub csv lines:  20714\n",
      "All lines:  176968 Sub csv lines:  12641\n"
     ]
    }
   ],
   "source": [
    "numofcsvs = 14\n",
    "\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    \n",
    "    text_csv = 'all_list/' + '%s_*.csv'%(lst)\n",
    "    text_csvs = glob.glob(text_csv)\n",
    "    \n",
    "    if len(text_csvs) == 0:\n",
    "        if os.path.exists('all_list/%s.csv'%(lst)):\n",
    "            texts = pd.read_csv('all_list/%s.csv'%(lst), index_col=0)\n",
    "        else:\n",
    "            data = xlrd3.open_workbook('/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx')\n",
    "            table = data.sheet_by_index(0)#.sheets()[0]          #通过索引顺序获取\n",
    "\n",
    "            # get full-text tweets\n",
    "            texts = []\n",
    "            if lst == 'first_2w':\n",
    "                test_idx = 5\n",
    "            elif lst == 'list2_img':\n",
    "                test_idx = 4\n",
    "            elif lst == 'list2_woimg':\n",
    "                test_idx = 6\n",
    "            else:\n",
    "                test_idx = 6\n",
    "\n",
    "            for i in range(1, table.nrows):\n",
    "                texts.append(table.row(i)[test_idx].value)\n",
    "\n",
    "            texts = pd.DataFrame(texts, columns=['text'])\n",
    "            texts.to_csv('all_list/%s.csv'%(lst))\n",
    "        \n",
    "        texts_len = len(texts)\n",
    "        csv_len = int(texts_len/numofcsvs)+1\n",
    "        print('All lines: ', texts_len, 'Sub csv lines: ', csv_len)\n",
    "        \n",
    "        for i in range(numofcsvs):\n",
    "            start = i * csv_len\n",
    "            end = min((i+1) * csv_len, texts_len)\n",
    "            if i+1 == numofcsvs:\n",
    "                end = texts_len\n",
    "                \n",
    "            texts[start:end].to_csv('all_list/%s_%d.csv'%(lst, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df43adc1-f8e7-43ec-bed4-dca71a1668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_csv = 'all_list/' + 'list2_*_*.csv'\n",
    "text_csvs = glob.glob(text_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fce45c-0317-4d14-ae73-76c61f295e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(processes=14)  # 创建nj个进程\n",
    "loc_csv = 'tianjin.csv'\n",
    "\n",
    "for tweet_csv in text_csvs:\n",
    "    # MatchProcess(tweet_csv, loc_csv, pickle_path)\n",
    "    pickle_path = tweet_csv.replace('csv', 'pickle')\n",
    "    pool.apply_async(MatchProcess, args=(tweet_csv, loc_csv, pickle_path))\n",
    "    \n",
    "pool.close()  # 关闭进程池，表示不能在往进程池中添加进程\n",
    "pool.join()  # 等待进程池中的所有进程执行完毕，必须在close()之后调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123ba8b8-ecb4-4dff-af61-152e9708ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.read_csv('all_list/%s_%d.csv'%('list2_img', 1), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdaead-d9a1-4f1e-9b8a-49d4b9563753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matirx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9dd11c1-3e7b-4c61-a857-d76c00b936e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.7644123073674 38.82259674787439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POINT (116.7644123073674 38.82259674787439)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.geometry[0].x, x.geometry[0].y) \n",
    "# str(x.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25507ac2-18c0-4f72-9204-0e31ffe4cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>元记电气焊补胎钣金</td>\n",
       "      <td>POINT (116.76441 38.82260)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>元汇津通卸载</td>\n",
       "      <td>POINT (116.75977 38.82440)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                    geometry\n",
       "0  元记电气焊补胎钣金  POINT (116.76441 38.82260)\n",
       "1     元汇津通卸载  POINT (116.75977 38.82440)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_names_loc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553f71b-2a4c-4670-b0c9-91b5aacdc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional cols\n",
    "with open('place2index.pickle', 'rb') as f:\n",
    "    [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "places_np = np.array(places)\n",
    "sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "append_cells = []\n",
    "for l in range(sparse_np.shape[1]):\n",
    "    row_cells = []\n",
    "    for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "        i_xy = names2loc[i]\n",
    "        row_cells.append([i, i_xy.x, i_xy.y])\n",
    "    append_cells.append(row_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3ed84-d46c-405b-a4f7-e349d07e0ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['酒店', 117.70100359480168, 39.296045987523456],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['天津瑞吉金融街酒店', 117.19148461137928, 39.12858938331741],\n",
       "  ['金融街', 117.20638076565724, 39.12375796277156]],\n",
       " [['饭', 117.54906841621506, 39.41817775960832]],\n",
       " [['小白楼', 117.20932596977684, 39.1159842943953],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['白楼', 117.28157823731766, 39.47219240381378],\n",
       "  ['西洋', 117.5147029707013, 39.03815707550861]],\n",
       " [['雾造', 116.91964343118586, 38.936991545703805],\n",
       "  ['津南区', 117.35020535412332, 38.93594862947365],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_cells[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b816d9fb-d817-4849-91b5-ee694969de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import xlwt\n",
    "from xlutils.copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78bb9984-21c2-4c48-8364-c76c7fea6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_loc = set(location_names.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "589da7e0-0d3f-401d-84ae-d373db7c01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for lst in ['first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index.pickle', 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    \n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "        \n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                         \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers),   label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fcae347c-5268-484c-a9c2-0c25b96351e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows:  5540\n"
     ]
    }
   ],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "for lst in ['noimg_6w']: #'first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index_%s.pickle'%(lst), 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    \n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "    \n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    nrows = read_table.nrows\n",
    "    print(\"Num of rows: \", nrows)\n",
    "    \n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                                 \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers), label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484219-63cb-4300-a29f-e0d5d8d97b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
