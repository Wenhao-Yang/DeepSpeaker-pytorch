{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bee833-3284-4cd5-8660-a84128879486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shapefile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xlrd3\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd169673-d272-42d8-9809-22aad949baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:38<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# read poi from shp files\n",
    "shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "places_names = []\n",
    "for f in tqdm(shp_files, ncols=200):\n",
    "    x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "    places_names.append(x.name)\n",
    "    \n",
    "names = pd.concat(places_names, axis=0)\n",
    "names.to_csv('tianjin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b03f67-5502-441b-bb64-dee69b845fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv('tianjin.csv').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcda49-97ee-40a5-8de3-39f32895c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenbelt = 'greenbelt'\n",
    "greenbelt_list = pd.read_excel('/home/yangwenhao/local/project/weibo_filter/app/data/' + greenbelt + '.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20ece0f1-b6cd-44e6-aaa8-27ce8ef76935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    元记电气焊补胎钣金\n",
       "1       元汇津通卸载\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae811b07-6a96-4c18-8f49-7b22a490434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_method(df, location):\n",
    "    return df.str.contains(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0865fa6e-9249-4e16-9d95-c2eac4f2cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_csv = 'all_list/' + '%s.csv'%('list2_img')\n",
    "for i, texts in enumerate(pd.read_csv(text_csv, chunksize=500000)):\n",
    "    texts.apply(apply_method, axis=0, location=\"天津\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff2e4db7-afd6-475f-bf0a-d2a08ee7879c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A Bite of Italy\" Four Hands Showcase at Four S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "12  A Bite of Italy\" Four Hands Showcase at Four S..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[12:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d14c45-9cf5-4540-90b2-f4f14fb427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_names = greenbelt_list.name # names\n",
    "location_names = names\n",
    "numofprint = 4\n",
    "\n",
    "# 'first_2w'\n",
    "# for lst in ['first_2w', 'noimg_6w']:\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    \n",
    "    text_csv = 'all_list/' + '%s.csv'%(lst)\n",
    "    if not os.path.exists(text_csv):\n",
    "        data = xlrd3.open_workbook('/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx')\n",
    "        table = data.sheet_by_index(0)#.sheets()[0]          #通过索引顺序获取\n",
    "\n",
    "        # get full-text tweets\n",
    "        texts = []\n",
    "        if lst == 'first_2w':\n",
    "            test_idx = 5\n",
    "        elif lst == 'list2_img':\n",
    "            test_idx = 4\n",
    "        elif lst == 'list2_woimg':\n",
    "            test_idx = 6\n",
    "        else:\n",
    "            test_idx = 6\n",
    "\n",
    "        for i in range(1, table.nrows):\n",
    "            texts.append(table.row(i)[test_idx].value)\n",
    "\n",
    "        texts = pd.DataFrame(texts, columns=['text'])\n",
    "        texts.to_csv('%s.csv'%(lst), index=False)\n",
    "    else:\n",
    "        texts = pd.read_csv(text_csv)\n",
    "    \n",
    "    # for i, texts in enumerate(pd.read_csv(text_csv, chunksize=50000)):\n",
    "    \n",
    "    # match all palces for all lines\n",
    "    places,sparse_matirx = [],[]\n",
    "    for idx, i in tqdm(enumerate(location_names.unique())): #[297604:]\n",
    "        try:\n",
    "            result = texts.text.str.contains(i) # 12:26:26\n",
    "            # result = texts.apply(apply_method, axis=0, location=i) # 17:11:27\n",
    "            # result = texts.parallel_apply(apply_method, axis=0, location=i) # 76:42:44\n",
    "            if result.to_numpy().sum() > 0:\n",
    "                places.append(i)\n",
    "                sparse_matirx.append(result.to_numpy())\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        if (idx+1) % 200000 == 0 or (idx+1) == len(location_names.unique()):\n",
    "            print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "            with open('all_list/place2index_%s_%d.pickle' % (lst,idx), 'wb') as f:\n",
    "                pickle.dump([places, sparse_matirx], f)\n",
    "            \n",
    "            places,sparse_matirx = [],[]\n",
    "\n",
    "    # save match results with pickle\n",
    "#     print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "#     # with open('all_list/place2index_%s_%d.pickle' % (lst,i), 'wb') as f:\n",
    "#     #     pickle.dump([places, sparse_matirx], f)\n",
    "#     with open('all_list/place2index_%s.pickle' % (lst), 'wb') as f:\n",
    "#         pickle.dump([places, sparse_matirx], f)\n",
    "        \n",
    "#     # print the most places in all tweets\n",
    "#     name2num = []\n",
    "#     for i,p in enumerate(places):\n",
    "#         name2num.append(sparse_matirx[i].sum())\n",
    "\n",
    "#     name2num = np.array(name2num)\n",
    "#     order_num = np.flip(np.argsort(name2num))\n",
    "#     np_places = np.array(places)\n",
    "#     for p, n in zip(np_places[order_num][:numofprint], name2num[order_num][:numofprint]):\n",
    "#         print(\"{:14<}\".format(p), \":\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bd5c6-6575-40e7-9807-f3d491ab6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save match results with pickle\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    \n",
    "    text_csv = 'all_list/' + '%s_*.csv'%(lst)\n",
    "    text_csvs = glob.glob(text_csv)\n",
    "\n",
    "    print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "    # with open('all_list/place2index_%s_%d.pickle' % (lst,i), 'wb') as f:\n",
    "    #     pickle.dump([places, sparse_matirx], f)\n",
    "    with open('all_list/  %s.pickle' % (lst), 'wb') as f:\n",
    "        pickle.dump([places, sparse_matirx], f)\n",
    "\n",
    "    # print the most places in all tweets\n",
    "    name2num = []\n",
    "    for i,p in enumerate(places):\n",
    "        name2num.append(sparse_matirx[i].sum())\n",
    "\n",
    "    name2num = np.array(name2num)\n",
    "    order_num = np.flip(np.argsort(name2num))\n",
    "    np_places = np.array(places)\n",
    "    for p, n in zip(np_places[order_num][:numofprint], name2num[order_num][:numofprint]):\n",
    "        print(\"{:14<}\".format(p), \":\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced0603a-1645-42b9-87e4-240c1fb54418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchProcess(tweet_csv, loc_csv, pickle_path):\n",
    "    texts = pd.read_csv(tweet_csv)\n",
    "    location_names = pd.read_csv(loc_csv).name\n",
    "    places,sparse_matirx = [],[]\n",
    "    \n",
    "    for i in tqdm(location_names.unique(), ncols=100): #[297604:]\n",
    "        try:\n",
    "            result = texts.text.str.contains(i)\n",
    "            if result.to_numpy().sum() > 0:\n",
    "                places.append(i)\n",
    "                sparse_matirx.append(result.to_numpy())\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "    print(tweet_csv, \"\\n 出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump([places, sparse_matirx], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8f0546a-6c20-4907-92db-4bbeed2fb295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All lines:  289992 Sub csv lines:  20714\n",
      "All lines:  176968 Sub csv lines:  12641\n"
     ]
    }
   ],
   "source": [
    "numofcsvs = 14\n",
    "\n",
    "for lst in ['list2_img', 'list2_woimg']:\n",
    "    \n",
    "    text_csv = 'all_list/' + '%s_*.csv'%(lst)\n",
    "    text_csvs = glob.glob(text_csv)\n",
    "    \n",
    "    if len(text_csvs) == 0:\n",
    "        if os.path.exists('all_list/%s.csv'%(lst)):\n",
    "            texts = pd.read_csv('all_list/%s.csv'%(lst), index_col=0)\n",
    "        else:\n",
    "            data = xlrd3.open_workbook('/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xlsx')\n",
    "            table = data.sheet_by_index(0)#.sheets()[0]          #通过索引顺序获取\n",
    "\n",
    "            # get full-text tweets\n",
    "            texts = []\n",
    "            if lst == 'first_2w':\n",
    "                test_idx = 5\n",
    "            elif lst == 'list2_img':\n",
    "                test_idx = 4\n",
    "            elif lst == 'list2_woimg':\n",
    "                test_idx = 6\n",
    "            else:\n",
    "                test_idx = 6\n",
    "\n",
    "            for i in range(1, table.nrows):\n",
    "                texts.append(table.row(i)[test_idx].value)\n",
    "\n",
    "            texts = pd.DataFrame(texts, columns=['text'])\n",
    "            texts.to_csv('all_list/%s.csv'%(lst))\n",
    "        \n",
    "        texts_len = len(texts)\n",
    "        csv_len = int(texts_len/numofcsvs)+1\n",
    "        print('All lines: ', texts_len, 'Sub csv lines: ', csv_len)\n",
    "        \n",
    "        for i in range(numofcsvs):\n",
    "            start = i * csv_len\n",
    "            end = min((i+1) * csv_len, texts_len)\n",
    "            if i+1 == numofcsvs:\n",
    "                end = texts_len\n",
    "                \n",
    "            texts[start:end].to_csv('all_list/%s_%d.csv'%(lst, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df43adc1-f8e7-43ec-bed4-dca71a1668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_csv = 'all_list/' + 'list2_*_*.csv'\n",
    "text_csvs = glob.glob(text_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fce45c-0317-4d14-ae73-76c61f295e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(processes=14)  # 创建nj个进程\n",
    "loc_csv = 'tianjin.csv'\n",
    "\n",
    "for tweet_csv in text_csvs:\n",
    "    # MatchProcess(tweet_csv, loc_csv, pickle_path)\n",
    "    pickle_path = tweet_csv.replace('csv', 'pickle')\n",
    "    pool.apply_async(MatchProcess, args=(tweet_csv, loc_csv, pickle_path))\n",
    "    \n",
    "pool.close()  # 关闭进程池，表示不能在往进程池中添加进程\n",
    "pool.join()  # 等待进程池中的所有进程执行完毕，必须在close()之后调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123ba8b8-ecb4-4dff-af61-152e9708ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pd.read_csv('all_list/%s_%d.csv'%('list2_img', 1), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdaead-d9a1-4f1e-9b8a-49d4b9563753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matirx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01484d6-b0a6-40fc-98e6-021b108cf125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:21<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# read poi and locations from shp files\n",
    "shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "places_names_loc = []\n",
    "for f in tqdm(shp_files):\n",
    "    # print(f)\n",
    "    x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "    places_names_loc.append(x[['name', 'geometry']])\n",
    "    \n",
    "places_names_loc = pd.concat(places_names_loc)\n",
    "names2loc = {}\n",
    "for l in places_names_loc.iterrows():\n",
    "    # print(l[1].values[0], l[1].values[1],)\n",
    "    names2loc[l[1].values[0]] = l[1].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9dd11c1-3e7b-4c61-a857-d76c00b936e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.7644123073674 38.82259674787439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POINT (116.7644123073674 38.82259674787439)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.geometry[0].x, x.geometry[0].y) \n",
    "# str(x.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25507ac2-18c0-4f72-9204-0e31ffe4cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>元记电气焊补胎钣金</td>\n",
       "      <td>POINT (116.76441 38.82260)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>元汇津通卸载</td>\n",
       "      <td>POINT (116.75977 38.82440)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                    geometry\n",
       "0  元记电气焊补胎钣金  POINT (116.76441 38.82260)\n",
       "1     元汇津通卸载  POINT (116.75977 38.82440)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_names_loc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553f71b-2a4c-4670-b0c9-91b5aacdc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional cols\n",
    "with open('place2index.pickle', 'rb') as f:\n",
    "    [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "places_np = np.array(places)\n",
    "sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "append_cells = []\n",
    "for l in range(sparse_np.shape[1]):\n",
    "    row_cells = []\n",
    "    for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "        i_xy = names2loc[i]\n",
    "        row_cells.append([i, i_xy.x, i_xy.y])\n",
    "    append_cells.append(row_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3ed84-d46c-405b-a4f7-e349d07e0ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['酒店', 117.70100359480168, 39.296045987523456],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['天津瑞吉金融街酒店', 117.19148461137928, 39.12858938331741],\n",
       "  ['金融街', 117.20638076565724, 39.12375796277156]],\n",
       " [['饭', 117.54906841621506, 39.41817775960832]],\n",
       " [['小白楼', 117.20932596977684, 39.1159842943953],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['白楼', 117.28157823731766, 39.47219240381378],\n",
       "  ['西洋', 117.5147029707013, 39.03815707550861]],\n",
       " [['雾造', 116.91964343118586, 38.936991545703805],\n",
       "  ['津南区', 117.35020535412332, 38.93594862947365],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_cells[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b816d9fb-d817-4849-91b5-ee694969de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import xlwt\n",
    "from xlutils.copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78bb9984-21c2-4c48-8364-c76c7fea6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_loc = set(location_names.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "589da7e0-0d3f-401d-84ae-d373db7c01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for lst in ['first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index.pickle', 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    \n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "        \n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                         \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers),   label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fcae347c-5268-484c-a9c2-0c25b96351e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows:  5540\n"
     ]
    }
   ],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "for lst in ['noimg_6w']: #'first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index_%s.pickle'%(lst), 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    \n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "    \n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    nrows = read_table.nrows\n",
    "    print(\"Num of rows: \", nrows)\n",
    "    \n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                                 \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers), label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484219-63cb-4300-a29f-e0d5d8d97b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
