{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bee833-3284-4cd5-8660-a84128879486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shapefile\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xlrd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd169673-d272-42d8-9809-22aad949baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:38<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# read poi from shp files\n",
    "shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "places_names = []\n",
    "for f in tqdm(shp_files):\n",
    "    x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "    places_names.append(x.name)\n",
    "    \n",
    "names = pd.concat(places_names, axis=0)\n",
    "\n",
    "names.to_csv('tianjin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b03f67-5502-441b-bb64-dee69b845fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv('tianjin.csv').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfcda49-97ee-40a5-8de3-39f32895c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "greenbelt = 'greenbelt'\n",
    "greenbelt_list = pd.read_excel('/home/yangwenhao/local/project/weibo_filter/app/data/' + greenbelt + '.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20ece0f1-b6cd-44e6-aaa8-27ce8ef76935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    元记电气焊补胎钣金\n",
       "1       元汇津通卸载\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5d14c45-9cf5-4540-90b2-f4f14fb427e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 801.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "天津动物园 : 15\n",
      "天津水上公园 : 15\n",
      "滨海文化中心 : 11\n",
      "水西公园 : 10\n",
      "西沽公园 : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 855.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "天津动物园 : 12\n",
      "滨海文化中心 : 9\n",
      "人民公园 : 7\n",
      "天津文化中心 : 5\n",
      "天津水上公园 : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "location_names = greenbelt_list.name # names\n",
    "\n",
    "# 'first_2w'\n",
    "for lst in ['first_2w', 'noimg_6w']:\n",
    "    data = xlrd3.open_workbook('/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls')\n",
    "    table = data.sheets()[0]          #通过索引顺序获取\n",
    "    \n",
    "    # get full-text tweets\n",
    "    texts = []\n",
    "    test_idx = 5 if lst == 'first_2w' else 6\n",
    "    for i in range(1, table.nrows):\n",
    "        texts.append(table.row(i)[test_idx].value)\n",
    "\n",
    "    texts = pd.DataFrame(texts, columns=['text'])\n",
    "    # texts.insert(1, 'places', pd.Series([[]]*len(texts)))\n",
    "    \n",
    "    # match all palces for all lines\n",
    "    places = []\n",
    "    sparse_matirx = []\n",
    "    for i in tqdm(location_names.unique()): #[297604:]\n",
    "        try:\n",
    "            result = texts.text.str.contains(i)\n",
    "            if result.to_numpy().sum() > 0:\n",
    "                places.append(i)\n",
    "                sparse_matirx.append(result.to_numpy())\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # save match results with pickle\n",
    "    print(\"出现的地点数: \", len(places), \" 记录的列数:\", len(sparse_matirx))\n",
    "    with open('place2index_%s_green.pickle' % lst, 'wb') as f:\n",
    "        pickle.dump([places, sparse_matirx], f)\n",
    "        \n",
    "    # print the most places in all tweets\n",
    "    name2num = []\n",
    "    for i,p in enumerate(places):\n",
    "        name2num.append(sparse_matirx[i].sum())\n",
    "\n",
    "    name2num = np.array(name2num)\n",
    "    order_num = np.flip(np.argsort(name2num))\n",
    "    np_places = np.array(places)\n",
    "    numofprint = 5\n",
    "    for p, n in zip(np_places[order_num][:numofprint], name2num[order_num][:numofprint]):\n",
    "        print(p, \":\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdaead-d9a1-4f1e-9b8a-49d4b9563753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_matirx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a01484d6-b0a6-40fc-98e6-021b108cf125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:22<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# read poi from shp files\n",
    "shp_files = glob.glob('/home/yangwenhao/local/project/weibo_filter/app/data/tianjin/*.shp')\n",
    "\n",
    "places_names_loc = []\n",
    "for f in tqdm(shp_files):\n",
    "    # print(f)\n",
    "    x = geopandas.read_file(f, engine=\"pyogrio\")\n",
    "    places_names_loc.append(x[['name', 'geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9dd11c1-3e7b-4c61-a857-d76c00b936e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.7644123073674 38.82259674787439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POINT (116.7644123073674 38.82259674787439)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.geometry[0].x, x.geometry[0].y) \n",
    "# str(x.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "438057f4-a636-4517-9367-205e0b9f2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_names_loc = pd.concat(places_names_loc)\n",
    "\n",
    "names2loc = {}\n",
    "for l in places_names_loc.iterrows():\n",
    "    # print(l[1].values[0], l[1].values[1],)\n",
    "    names2loc[l[1].values[0]] = l[1].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25507ac2-18c0-4f72-9204-0e31ffe4cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>元记电气焊补胎钣金</td>\n",
       "      <td>POINT (116.76441 38.82260)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>元汇津通卸载</td>\n",
       "      <td>POINT (116.75977 38.82440)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                    geometry\n",
       "0  元记电气焊补胎钣金  POINT (116.76441 38.82260)\n",
       "1     元汇津通卸载  POINT (116.75977 38.82440)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_names_loc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553f71b-2a4c-4670-b0c9-91b5aacdc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read additional cols\n",
    "with open('place2index.pickle', 'rb') as f:\n",
    "    [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "places_np = np.array(places)\n",
    "sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "append_cells = []\n",
    "for l in range(sparse_np.shape[1]):\n",
    "    row_cells = []\n",
    "    for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "        i_xy = names2loc[i]\n",
    "        row_cells.append([i, i_xy.x, i_xy.y])\n",
    "    append_cells.append(row_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3ed84-d46c-405b-a4f7-e349d07e0ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['酒店', 117.70100359480168, 39.296045987523456],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['天津瑞吉金融街酒店', 117.19148461137928, 39.12858938331741],\n",
       "  ['金融街', 117.20638076565724, 39.12375796277156]],\n",
       " [['饭', 117.54906841621506, 39.41817775960832]],\n",
       " [['小白楼', 117.20932596977684, 39.1159842943953],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583],\n",
       "  ['白楼', 117.28157823731766, 39.47219240381378],\n",
       "  ['西洋', 117.5147029707013, 39.03815707550861]],\n",
       " [['雾造', 116.91964343118586, 38.936991545703805],\n",
       "  ['津南区', 117.35020535412332, 38.93594862947365],\n",
       "  ['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]],\n",
       " [['天津', 117.17989775825995, 39.12978780211583]]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_cells[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b816d9fb-d817-4849-91b5-ee694969de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import xlwt\n",
    "from xlutils.copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78bb9984-21c2-4c48-8364-c76c7fea6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_loc = set(location_names.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "589da7e0-0d3f-401d-84ae-d373db7c01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for lst in ['first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index.pickle', 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    \n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "        \n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                         \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers),   label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fcae347c-5268-484c-a9c2-0c25b96351e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows:  5540\n"
     ]
    }
   ],
   "source": [
    "headers = [\"相关地点数量\", \"微博id\", \"微博主页\", \"文本\", \"日期\", \"图片\", \"转发数\", \"评论数\", \"点赞数\"]\n",
    "read_cols = [2, 3, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "for lst in ['noimg_6w']: #'first_2w']: #, \n",
    "    # read additional cols\n",
    "    with open('place2index_%s.pickle'%(lst), 'rb') as f:\n",
    "        [places, sparse_matirx] = pickle.load(f)\n",
    "\n",
    "    places_np = np.array(places)\n",
    "    sparse_np = np.array(sparse_matirx) #.shape\n",
    "\n",
    "    append_cells = []\n",
    "    for l in range(sparse_np.shape[1]):\n",
    "        row_cells = []\n",
    "        for i in places_np[np.where(sparse_np[:, l] == True)[0]]:\n",
    "            i_xy = names2loc[i]\n",
    "            row_cells.append([i, i_xy.x, i_xy.y])\n",
    "        append_cells.append(row_cells)\n",
    "    \n",
    "    max_los = 1\n",
    "    for i in append_cells:\n",
    "        max_los = max(max_los, len(i))\n",
    "    \n",
    "    read_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '.xls'\n",
    "    save_lst = r'/home/yangwenhao/local/project/weibo_filter/app/data/' + lst + '_loc.xls'\n",
    "\n",
    "    \n",
    "    # writebook = copy(wb=read_writebook)\n",
    "    # writesheet = writebook.get_sheet(0)\n",
    "    \n",
    "    writebook = xlwt.Workbook(encoding='utf-8')\n",
    "    # 添加sheet 写入excel, 参数对应 行, 列, 值\n",
    "    poi_sheet     = writebook.add_sheet(u'地点检索到公园POI',   cell_overwrite_ok=True)\n",
    "    npoi_sheet    = writebook.add_sheet(u'地点检索到非公园POI', cell_overwrite_ok=True)\n",
    "    tianjin_sheet = writebook.add_sheet(u'地点仅检索到天津',    cell_overwrite_ok=True)\n",
    "    \n",
    "    poi_row, npoi_row, tianjin_row = 1, 1, 1\n",
    "    \n",
    "# 相关地点数量\t微博id\t微博主页\t文本\t日期\t图片\t转发数\t评论数\t点赞数\n",
    "    for sheet in [poi_sheet, npoi_sheet, tianjin_sheet]:\n",
    "        for i, h in enumerate(headers):\n",
    "            sheet.write(0, i, label=h)\n",
    "    \n",
    "    for sheet in [poi_sheet, npoi_sheet]:\n",
    "        for i in range(1, max_los+1):\n",
    "            sheet.write(0, (i-1)*3+len(headers),   label='地点提取%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+1, label='经度%d'%i)\n",
    "            sheet.write(0, (i-1)*3+len(headers)+2, label='纬度%d'%i)\n",
    "    \n",
    "    tianjin_sheet.write(0, len(headers), label='地点提取')\n",
    "    \n",
    "    read_writebook = xlrd.open_workbook(filename=read_lst)\n",
    "    read_table = read_writebook.sheet_by_index(0)\n",
    "    nrows = read_table.nrows\n",
    "    print(\"Num of rows: \", nrows)\n",
    "    \n",
    "    for i in range(1, nrows):\n",
    "        this_line = read_table.row_slice(i)\n",
    "        this_locs = append_cells[i-1]\n",
    "        locs_names = [l[0] for l in this_locs]\n",
    "        \n",
    "        green_tweet = False\n",
    "        for l in locs_names:\n",
    "            if l in green_loc:\n",
    "                green_tweet = True\n",
    "                break\n",
    "                \n",
    "        if len(locs_names) == 1 and locs_names[0] == '天津':\n",
    "            writesheet = tianjin_sheet\n",
    "            this_row = tianjin_row\n",
    "            tianjin_row += 1\n",
    "        elif green_tweet:\n",
    "            writesheet = poi_sheet\n",
    "            this_row = poi_row\n",
    "            poi_row += 1\n",
    "        else:\n",
    "            writesheet = npoi_sheet\n",
    "            this_row = npoi_row\n",
    "            npoi_row += 1\n",
    "        \n",
    "        writesheet.write(this_row, 0, label=len(append_cells[i-1]))\n",
    "                                 \n",
    "        for j, c in enumerate(read_cols):\n",
    "            writesheet.write(this_row, j+1, label=this_line[c].value)\n",
    "        \n",
    "        for j, (name, x, y) in enumerate(append_cells[i-1]):\n",
    "            writesheet.write(this_row, j*3+len(headers), label=name)\n",
    "            writesheet.write(this_row, j*3+len(headers)+1, label=x)\n",
    "            writesheet.write(this_row, j*3+len(headers)+2, label=y)\n",
    "\n",
    "    writebook.save(save_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40484219-63cb-4300-a29f-e0d5d8d97b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
