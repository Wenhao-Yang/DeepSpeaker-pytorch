{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe8b0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb6842a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('My app')\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c410e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 71, Lines with b: 38\n"
     ]
    }
   ],
   "source": [
    "logFile = \"file:///usr/local/spark-3.3.1-bin-hadoop3/README.md\"\n",
    "logData = sc.textFile(logFile, 2).cache()\n",
    "numAs = logData.filter(lambda line: 'a' in line).count()\n",
    "numBs = logData.filter(lambda line: 'b' in line).count()\n",
    "print('Lines with a: %s, Lines with b: %s' % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a595d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "('#', 1)('a', 9)                                                    (0 + 2) / 2]\n",
      "\n",
      "('for', 13)('Apache', 1)\n",
      "\n",
      "('large-scale', 1)('Spark', 15)\n",
      "\n",
      "('data', 2)('', 41)\n",
      "\n",
      "('processing.', 2)('is', 7)\n",
      "\n",
      "('Python,', 2)('unified', 1)\n",
      "\n",
      "('and', 9)\n",
      "('analytics', 1)\n",
      "('R,', 1)\n",
      "('engine', 2)\n",
      "('that', 2)\n",
      "('It', 2)\n",
      "('general', 2)\n",
      "('provides', 1)\n",
      "('graphs', 1)\n",
      "('high-level', 1)\n",
      "('also', 5)\n",
      "('APIs', 1)\n",
      "('rich', 1)\n",
      "('in', 5)\n",
      "('higher-level', 1)\n",
      "('Scala,', 1)\n",
      "('including', 4)\n",
      "('Java,', 1)\n",
      "('DataFrames,', 1)\n",
      "('an', 4)\n",
      "('API', 1)\n",
      "('optimized', 1)\n",
      "('on', 8)\n",
      "('supports', 2)\n",
      "('workloads,', 1)\n",
      "('computation', 1)\n",
      "('Streaming', 1)\n",
      "('analysis.', 1)\n",
      "('stream', 1)\n",
      "('set', 2)\n",
      "('[![GitHub', 1)\n",
      "('of', 5)\n",
      "('Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush)', 1)('tools', 1)\n",
      "\n",
      "('SQL', 2)('[![AppVeyor', 1)\n",
      "\n",
      "('pandas', 2)('[![PySpark', 1)\n",
      "\n",
      "('MLlib', 1)('Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', 1)\n",
      "\n",
      "('machine', 1)('##', 9)\n",
      "\n",
      "('learning,', 1)('Online', 1)\n",
      "\n",
      "('GraphX', 1)('You', 3)\n",
      "\n",
      "('graph', 1)('can', 6)\n",
      "\n",
      "('processing,', 1)('find', 1)\n",
      "\n",
      "('Structured', 1)('the', 23)\n",
      "\n",
      "('<https://spark.apache.org/>', 1)('documentation,', 1)\n",
      "\n",
      "('Action', 1)('web', 1)\n",
      "\n",
      "('This', 2)('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 1)\n",
      "\n",
      "('file', 1)('Documentation', 1)\n",
      "\n",
      "('contains', 1)('latest', 1)\n",
      "\n",
      "('setup', 1)('programming', 1)\n",
      "\n",
      "('built', 1)('guide,', 1)\n",
      "\n",
      "('Maven](https://maven.apache.org/).', 1)('[project', 1)\n",
      "\n",
      "('To', 2)('page](https://spark.apache.org/documentation.html).', 1)\n",
      "\n",
      "('build', 3)('README', 1)\n",
      "\n",
      "('its', 1)('only', 1)\n",
      "\n",
      "('example', 3)('basic', 1)\n",
      "\n",
      "('programs,', 1)('instructions.', 1)\n",
      "\n",
      "('```bash', 6)('Building', 1)\n",
      "\n",
      "('using', 3)('./build/mvn', 1)\n",
      "\n",
      "('[Apache', 1)('-DskipTests', 1)\n",
      "\n",
      "('run:', 1)('clean', 1)\n",
      "\n",
      "('do', 2)('package', 1)\n",
      "\n",
      "('this', 1)('```', 8)\n",
      "\n",
      "('downloaded', 1)('(You', 1)\n",
      "\n",
      "('documentation', 3)('not', 1)\n",
      "\n",
      "('project', 1)('need', 1)\n",
      "\n",
      "('site,', 1)('to', 16)\n",
      "\n",
      "('at', 2)('if', 4)\n",
      "\n",
      "('you', 4)('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1)\n",
      "\n",
      "('pre-built', 1)('development', 1)\n",
      "\n",
      "('package.)', 1)('tips,', 1)\n",
      "\n",
      "('More', 1)('developing', 1)\n",
      "\n",
      "('detailed', 2)('IDE,', 1)\n",
      "\n",
      "('available', 1)('[\"Useful', 1)\n",
      "\n",
      "('Developer', 1)('from', 1)\n",
      "\n",
      "('Interactive', 2)('[\"Building', 1)\n",
      "\n",
      "('Shell', 2)('For', 3)\n",
      "\n",
      "('The', 1)('info', 1)\n",
      "\n",
      "('way', 1)('see', 3)\n",
      "\n",
      "('start', 1)('Tools\"](https://spark.apache.org/developer-tools.html).', 1)\n",
      "\n",
      "('Try', 1)\n",
      "('Scala', 2)\n",
      "('following', 2)\n",
      "('easiest', 1)\n",
      "('scala>', 1)\n",
      "('through', 1)\n",
      "('spark.range(1000', 2)\n",
      "('shell:', 2)\n",
      "('*', 4)\n",
      "('./bin/spark-shell', 1)\n",
      "('1000).count()', 2)\n",
      "('command,', 2)\n",
      "('Python', 2)\n",
      "('which', 2)\n",
      "('Alternatively,', 1)\n",
      "('should', 2)('use', 3)\n",
      "\n",
      "('return', 2)('And', 1)\n",
      "\n",
      "('1,000,000,000:', 2)('run', 7)\n",
      "\n",
      "('```scala', 1)('```python', 1)\n",
      "\n",
      "('1000', 2)('Example', 1)\n",
      "\n",
      "('prefer', 1)('several', 1)\n",
      "\n",
      "('./bin/pyspark', 1)('programs', 2)\n",
      "\n",
      "('>>>', 1)('them,', 1)\n",
      "\n",
      "('Programs', 1)('`./bin/run-example', 1)\n",
      "\n",
      "('comes', 1)('[params]`.', 1)\n",
      "\n",
      "('with', 3)('example:', 1)\n",
      "('sample', 1)\n",
      "('`examples`', 2)\n",
      "('directory.', 1)\n",
      "\n",
      "('one', 2)('./bin/run-example', 2)\n",
      "\n",
      "('<class>', 1)('SparkPi', 2)\n",
      "\n",
      "('will', 1)('variable', 1)\n",
      "\n",
      "('Pi', 1)('when', 1)\n",
      "\n",
      "('locally.', 1)('examples', 2)\n",
      "\n",
      "('MASTER', 1)('spark://', 1)\n",
      "\n",
      "('environment', 1)('URL,', 1)\n",
      "\n",
      "('running', 1)('YARN,', 1)\n",
      "\n",
      "('submit', 1)('\"local\"', 1)\n",
      "\n",
      "('cluster.', 1)('locally', 2)\n",
      "\n",
      "('be', 2)('N', 1)\n",
      "\n",
      "('mesos://', 1)('abbreviated', 1)\n",
      "\n",
      "('or', 3)('class', 2)\n",
      "\n",
      "('\"yarn\"', 1)\n",
      "('name', 1)\n",
      "('thread,', 1)('package.', 1)\n",
      "\n",
      "('\"local[N]\"', 1)('instance:', 1)\n",
      "\n",
      "('threads.', 1)('print', 1)\n",
      "\n",
      "('MASTER=spark://host:7077', 1)('usage', 1)\n",
      "\n",
      "('Many', 1)('help', 1)\n",
      "\n",
      "('given.', 1)('no', 1)\n",
      "\n",
      "('Running', 1)('params', 1)\n",
      "\n",
      "('Tests', 1)('are', 1)\n",
      "\n",
      "('first', 1)('Testing', 1)\n",
      "\n",
      "('requires', 1)('Spark](#building-spark).', 1)\n",
      "\n",
      "('Once', 1)('[building', 1)\n",
      "\n",
      "('built,', 1)('how', 3)\n",
      "\n",
      "('tests', 2)('[run', 1)\n",
      "\n",
      "('using:', 1)('tests](https://spark.apache.org/developer-tools.html#individual-tests).', 1)\n",
      "\n",
      "('./dev/run-tests', 1)('There', 1)\n",
      "\n",
      "('Please', 4)('Kubernetes', 1)\n",
      "\n",
      "('guidance', 2)('resource-managers/kubernetes/integration-tests/README.md', 1)\n",
      "\n",
      "('module,', 1)('A', 1)\n",
      "\n",
      "('individual', 1)('Hadoop', 3)\n",
      "\n",
      "('integration', 1)('Versions', 1)\n",
      "\n",
      "('test,', 1)('core', 1)\n",
      "\n",
      "('talk', 1)('Note', 1)\n",
      "\n",
      "('About', 1)('protocols', 1)\n",
      "\n",
      "('uses', 1)('same', 1)\n",
      "\n",
      "('library', 1)('your', 1)\n",
      "\n",
      "('HDFS', 1)('cluster', 1)\n",
      "\n",
      "('other', 1)('runs.', 1)\n",
      "\n",
      "('Hadoop-supported', 1)('[\"Specifying', 1)\n",
      "\n",
      "('storage', 1)('Version', 1)\n",
      "\n",
      "('systems.', 1)('Enabling', 1)\n",
      "\n",
      "('Because', 1)('building', 2)\n",
      "\n",
      "('have', 1)('Configuration', 1)\n",
      "\n",
      "('changed', 1)('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1)\n",
      "\n",
      "('different', 1)('review', 1)\n",
      "\n",
      "('versions', 1)('[Contribution', 1)\n",
      "\n",
      "('Hadoop,', 2)('information', 1)\n",
      "\n",
      "('must', 1)('get', 1)\n",
      "\n",
      "('against', 1)\n",
      "('version', 1)\n",
      "('refer', 2)\n",
      "('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 1)\n",
      "('particular', 2)\n",
      "('distribution', 1)\n",
      "('Hive', 2)\n",
      "('Thriftserver', 1)\n",
      "('distributions.', 1)\n",
      "('[Configuration', 1)\n",
      "('online', 1)\n",
      "('overview', 1)\n",
      "('configure', 1)\n",
      "('Spark.', 1)\n",
      "('Contributing', 1)\n",
      "('guide](https://spark.apache.org/contributing.html)', 1)\n",
      "('started', 1)\n",
      "('contributing', 1)\n",
      "('project.', 1)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的模块\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# 创建 SparkContext对象\n",
    "sc = SparkContext(conf=SparkConf())\n",
    "\n",
    "# 读取文本文件，并创建 RDD \n",
    "lines = sc.textFile(\"file:///usr/local/spark-3.3.1-bin-hadoop3/README.md\")\n",
    "# 对文本文件中的每一行进行单词计数\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                .map(lambda word: (word, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "# 打印每一行结果\n",
    "counts.foreach(print)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb9b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('a', 9)('#', 1)\n",
      "\n",
      "('Apache', 1)('for', 13)\n",
      "\n",
      "('Spark', 15)('large-scale', 1)\n",
      "\n",
      "('', 41)('data', 2)\n",
      "\n",
      "('is', 7)('processing.', 2)\n",
      "\n",
      "('unified', 1)('Python,', 2)\n",
      "\n",
      "('analytics', 1)('and', 9)\n",
      "\n",
      "('engine', 2)('R,', 1)\n",
      "\n",
      "('It', 2)('that', 2)\n",
      "\n",
      "('provides', 1)('general', 2)\n",
      "\n",
      "('high-level', 1)('graphs', 1)\n",
      "\n",
      "('also', 5)('APIs', 1)\n",
      "\n",
      "('rich', 1)('in', 5)\n",
      "\n",
      "('higher-level', 1)('Scala,', 1)\n",
      "\n",
      "('including', 4)('Java,', 1)\n",
      "\n",
      "('DataFrames,', 1)('an', 4)\n",
      "\n",
      "('API', 1)('optimized', 1)\n",
      "\n",
      "('on', 8)('supports', 2)\n",
      "\n",
      "('workloads,', 1)('computation', 1)\n",
      "\n",
      "('Streaming', 1)('analysis.', 1)\n",
      "\n",
      "('stream', 1)('set', 2)\n",
      "\n",
      "('[![GitHub', 1)('of', 5)\n",
      "\n",
      "('Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster+event%3Apush)', 1)('tools', 1)\n",
      "\n",
      "('[![AppVeyor', 1)('SQL', 2)\n",
      "\n",
      "('[![PySpark', 1)('pandas', 2)\n",
      "\n",
      "('MLlib', 1)('Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)', 1)\n",
      "\n",
      "('machine', 1)('##', 9)\n",
      "\n",
      "('learning,', 1)('Online', 1)\n",
      "\n",
      "('GraphX', 1)('You', 3)\n",
      "\n",
      "('graph', 1)('can', 6)\n",
      "\n",
      "('processing,', 1)('find', 1)\n",
      "\n",
      "('Structured', 1)('the', 23)\n",
      "\n",
      "('<https://spark.apache.org/>', 1)('documentation,', 1)\n",
      "\n",
      "('Action', 1)('web', 1)\n",
      "\n",
      "('This', 2)('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)', 1)\n",
      "\n",
      "('file', 1)('Documentation', 1)\n",
      "\n",
      "('contains', 1)('latest', 1)\n",
      "\n",
      "('setup', 1)('programming', 1)\n",
      "\n",
      "('built', 1)('guide,', 1)\n",
      "\n",
      "('Maven](https://maven.apache.org/).', 1)('[project', 1)\n",
      "\n",
      "('To', 2)('page](https://spark.apache.org/documentation.html).', 1)\n",
      "\n",
      "('build', 3)('README', 1)\n",
      "\n",
      "('its', 1)('only', 1)\n",
      "\n",
      "('example', 3)('basic', 1)\n",
      "\n",
      "('programs,', 1)('instructions.', 1)\n",
      "\n",
      "('Building', 1)('```bash', 6)\n",
      "\n",
      "('./build/mvn', 1)('using', 3)\n",
      "\n",
      "('-DskipTests', 1)('[Apache', 1)\n",
      "\n",
      "('clean', 1)('run:', 1)\n",
      "\n",
      "('package', 1)('do', 2)\n",
      "\n",
      "('```', 8)('this', 1)\n",
      "\n",
      "('(You', 1)('downloaded', 1)\n",
      "\n",
      "('not', 1)('documentation', 3)\n",
      "\n",
      "('need', 1)('project', 1)\n",
      "\n",
      "('to', 16)('site,', 1)\n",
      "\n",
      "('if', 4)('at', 2)\n",
      "\n",
      "('you', 4)('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1)\n",
      "\n",
      "('pre-built', 1)('development', 1)\n",
      "\n",
      "('package.)', 1)('tips,', 1)\n",
      "\n",
      "('More', 1)('developing', 1)\n",
      "\n",
      "('detailed', 2)('IDE,', 1)\n",
      "\n",
      "('available', 1)\n",
      "('[\"Useful', 1)\n",
      "('from', 1)\n",
      "('Developer', 1)\n",
      "('[\"Building', 1)\n",
      "('Interactive', 2)\n",
      "('For', 3)\n",
      "('Shell', 2)\n",
      "('info', 1)('The', 1)\n",
      "\n",
      "('see', 3)('way', 1)\n",
      "\n",
      "('Tools\"](https://spark.apache.org/developer-tools.html).', 1)('start', 1)\n",
      "\n",
      "('Scala', 2)('Try', 1)\n",
      "\n",
      "('easiest', 1)('following', 2)\n",
      "\n",
      "('through', 1)('scala>', 1)\n",
      "\n",
      "('shell:', 2)('spark.range(1000', 2)\n",
      "\n",
      "('./bin/spark-shell', 1)('*', 4)\n",
      "\n",
      "('command,', 2)('1000).count()', 2)\n",
      "\n",
      "('which', 2)('Python', 2)\n",
      "\n",
      "('should', 2)('Alternatively,', 1)\n",
      "\n",
      "('return', 2)('use', 3)\n",
      "\n",
      "('1,000,000,000:', 2)('And', 1)\n",
      "\n",
      "('```scala', 1)('run', 7)\n",
      "\n",
      "('```python', 1)('1000', 2)\n",
      "\n",
      "('Example', 1)('prefer', 1)\n",
      "\n",
      "('several', 1)('./bin/pyspark', 1)\n",
      "\n",
      "('programs', 2)('>>>', 1)\n",
      "\n",
      "('them,', 1)('Programs', 1)\n",
      "\n",
      "('`./bin/run-example', 1)('comes', 1)\n",
      "\n",
      "('[params]`.', 1)('with', 3)\n",
      "\n",
      "('example:', 1)('sample', 1)\n",
      "\n",
      "('./bin/run-example', 2)('`examples`', 2)\n",
      "\n",
      "('SparkPi', 2)('directory.', 1)\n",
      "\n",
      "('variable', 1)('one', 2)\n",
      "\n",
      "('when', 1)('<class>', 1)\n",
      "\n",
      "('examples', 2)('will', 1)\n",
      "\n",
      "('spark://', 1)('Pi', 1)\n",
      "\n",
      "('URL,', 1)('locally.', 1)\n",
      "\n",
      "('YARN,', 1)('MASTER', 1)\n",
      "\n",
      "('\"local\"', 1)('environment', 1)\n",
      "\n",
      "('locally', 2)('running', 1)\n",
      "\n",
      "('N', 1)('submit', 1)\n",
      "\n",
      "('abbreviated', 1)('cluster.', 1)\n",
      "\n",
      "('class', 2)('be', 2)\n",
      "\n",
      "('mesos://', 1)('name', 1)\n",
      "\n",
      "('or', 3)('package.', 1)\n",
      "\n",
      "('\"yarn\"', 1)('instance:', 1)\n",
      "\n",
      "('thread,', 1)('print', 1)\n",
      "\n",
      "('\"local[N]\"', 1)('usage', 1)\n",
      "\n",
      "('threads.', 1)('help', 1)\n",
      "\n",
      "('MASTER=spark://host:7077', 1)('no', 1)\n",
      "\n",
      "('Many', 1)('params', 1)\n",
      "\n",
      "('given.', 1)('are', 1)\n",
      "\n",
      "('Running', 1)('Testing', 1)\n",
      "\n",
      "('Tests', 1)('Spark](#building-spark).', 1)\n",
      "\n",
      "('first', 1)('Once', 1)\n",
      "\n",
      "('requires', 1)('built,', 1)\n",
      "\n",
      "('tests', 2)\n",
      "('using:', 1)\n",
      "('./dev/run-tests', 1)\n",
      "('Please', 4)\n",
      "('guidance', 2)\n",
      "('module,', 1)\n",
      "('individual', 1)\n",
      "('integration', 1)\n",
      "('test,', 1)\n",
      "('Note', 1)\n",
      "('About', 1)\n",
      "('uses', 1)\n",
      "('library', 1)\n",
      "('HDFS', 1)\n",
      "('other', 1)\n",
      "('Hadoop-supported', 1)('[building', 1)\n",
      "\n",
      "('storage', 1)\n",
      "('how', 3)\n",
      "('systems.', 1)\n",
      "('[run', 1)\n",
      "('Because', 1)\n",
      "('tests](https://spark.apache.org/developer-tools.html#individual-tests).', 1)('have', 1)\n",
      "\n",
      "('There', 1)('changed', 1)\n",
      "\n",
      "('Kubernetes', 1)('different', 1)\n",
      "\n",
      "('resource-managers/kubernetes/integration-tests/README.md', 1)('versions', 1)\n",
      "\n",
      "('A', 1)('Hadoop,', 2)\n",
      "\n",
      "('Hadoop', 3)('must', 1)\n",
      "\n",
      "('Versions', 1)('against', 1)\n",
      "\n",
      "('core', 1)('version', 1)\n",
      "\n",
      "('talk', 1)('refer', 2)\n",
      "\n",
      "('protocols', 1)\n",
      "('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 1)\n",
      "('same', 1)\n",
      "('particular', 2)\n",
      "('your', 1)\n",
      "('distribution', 1)\n",
      "('cluster', 1)\n",
      "('Hive', 2)\n",
      "('runs.', 1)\n",
      "('Thriftserver', 1)\n",
      "('[\"Specifying', 1)\n",
      "('distributions.', 1)\n",
      "('Version', 1)('[Configuration', 1)\n",
      "\n",
      "('Enabling', 1)\n",
      "('online', 1)\n",
      "('building', 2)\n",
      "('overview', 1)\n",
      "('Configuration', 1)\n",
      "('configure', 1)\n",
      "('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1)('Spark.', 1)\n",
      "\n",
      "('review', 1)('Contributing', 1)\n",
      "\n",
      "('[Contribution', 1)('guide](https://spark.apache.org/contributing.html)', 1)\n",
      "\n",
      "('information', 1)('started', 1)\n",
      "\n",
      "('get', 1)('contributing', 1)\n",
      "\n",
      "('project.', 1)\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"hdfs://node1.sparklab.cn:9000/user/yangwenhao/README.md\")\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                .map(lambda word: (word, 1)) \\\n",
    "                .reduceByKey(lambda a, b: a + b)\n",
    "        \n",
    "# 打印每一行结果\n",
    "counts.foreach(print)\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093e4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
