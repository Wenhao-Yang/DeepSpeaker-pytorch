{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0206358a-de5f-4d48-b9b7-fc32e180e166",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6489aebd-486c-45f0-8cbb-dd4f66482db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/anaconda3/envs/cords/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-20 01:20:52,295\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-20 01:20:52,361\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from cords.utils.data.datasets.SL import gen_dataset\n",
    "from torch.utils.data import Subset\n",
    "from cords.utils.config_utils import load_config_data\n",
    "from cords.utils.data.data_utils import WeightedSubset\n",
    "from ray import tune\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os.path as osp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000364f3-0870-48fa-b72c-be483005a55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Light.dataset import Sampler_Loaders, SubDatasets, SubScriptDatasets, SubLoaders\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from Light.model import SpeakerLoss\n",
    "from Define_Model.Optimizer import EarlyStopping\n",
    "\n",
    "from TrainAndTest.common_func import create_classifier, create_optimizer, create_scheduler, create_model, verification_test, verification_extract, \\\n",
    "    args_parse, args_model, save_model_args\n",
    "\n",
    "from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, GradMatchDataLoader\n",
    "\n",
    "#, OLRandomDataLoader, \\\n",
    "    # CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader\n",
    "from dotmap import DotMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a5721a-b4d0-438c-8ace-737ebb9a8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_logger(results_dir):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    # setup logger\n",
    "    plain_formatter = logging.Formatter(\"[%(asctime)s] %(name)s %(levelname)s: %(message)s\",\n",
    "                                      datefmt=\"%m/%d %H:%M:%S\")\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    s_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "    s_handler.setFormatter(plain_formatter)\n",
    "    s_handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(s_handler)\n",
    "    f_handler = logging.FileHandler(os.path.join(results_dir, \"results.log\"))\n",
    "    f_handler.setFormatter(plain_formatter)\n",
    "    f_handler.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(f_handler)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "         mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing / 3600\n",
    "\n",
    "def save_ckpt(state, ckpt_path):\n",
    "    torch.save(state, ckpt_path)\n",
    "\n",
    "def load_ckpt(ckpt_path, model, optimizer):\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss = checkpoint['loss']\n",
    "    metrics = checkpoint['metrics']\n",
    "    return start_epoch, model, optimizer, loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f08dc32-b05f-4455-951c-a47c9b0fb60a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6425c8c0-36c9-4dfc-bf1c-2ec0892141e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/yangwenhao/project/SpeakerVerification-pytorch'\n",
    "lstm_dir = '/home/yangwenhao/project/lstm_speaker_verification/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d6cbd9-4e0e-466b-ba09-6b93446450d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6396e7b9-961f-421d-8111-bf5b4279b48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_config = data_dir + '/Data/checkpoint/ECAPA_brain/Mean_batch96_SASP2_em192_official_2sesmix8/arcsoft_adam_cyclic/vox1/wave_fb80_band05_aug5/123456/model.2023.12.17.yaml'\n",
    "\n",
    "train_config = 'model.2023.12.17.yaml'\n",
    "\n",
    "with open(train_config, 'r') as f:\n",
    "    config_args = load_hyperpyyaml(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c1a7a-bbad-4daf-853f-9c76060ed57b",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "468325e9-cd0c-4310-9db1-b15136d6613f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Generating 1 lengths with Average: 32000.\n"
     ]
    }
   ],
   "source": [
    "train_dir, valid_dir, train_extract_dir = SubScriptDatasets(config_args)\n",
    "# train_loader, train_sampler, valid_loader, valid_sampler, train_extract_loader, train_extract_sampler = Sampler_Loaders(\n",
    "#         train_dir, valid_dir, train_extract_dir, config_args)\n",
    "# train_dir.base_utts = train_dir.base_utts[:153600]\n",
    "train_loader, valid_loader, train_extract_loader = SubLoaders(train_dir, valid_dir, train_extract_dir, config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3b7fc91-bc60-4cc3-8181-151409fa6991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:2' #Device Argument\n",
    "\n",
    "model = config_args['embedding_model']\n",
    "\n",
    "model.loss = SpeakerLoss(config_args)\n",
    "model.loss.reduction = 'none'\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1ac529-ad2b-42a0-a945-2dfbfcb5c28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the lr and weight_decay of classifier to 0.001000 and 0.000200\n"
     ]
    }
   ],
   "source": [
    "model_para = [{'params': model.parameters()}]\n",
    "if config_args['loss_type'] in ['center', 'variance', 'mulcenter', 'gaussian', 'coscenter', 'ring']:\n",
    "    assert config_args['lr_ratio'] > 0\n",
    "    model_para.append({'params': model.loss.xe_criterion.parameters(\n",
    "    ), 'lr': config_args['lr'] * config_args['lr_ratio']})\n",
    "\n",
    "if 'second_wd' in config_args and config_args['second_wd'] > 0:\n",
    "    # if config_args['loss_type in ['asoft', 'amsoft']:\n",
    "    classifier_params = list(map(id, model.classifier.parameters()))\n",
    "    rest_params = filter(lambda p: id(\n",
    "        p) not in classifier_params, model.parameters())\n",
    "\n",
    "    init_lr = config_args['lr'] * \\\n",
    "        config_args['lr_ratio'] if config_args['lr_ratio'] > 0 else config_args['lr']\n",
    "    init_wd = config_args['second_wd'] if config_args['second_wd'] > 0 else config_args['weight_decay']\n",
    "    print('Set the lr and weight_decay of classifier to %f and %f' %\n",
    "          (init_lr, init_wd))\n",
    "    model_para = [{'params': rest_params},\n",
    "                  {'params': model.classifier.parameters(), 'lr': init_lr, 'weight_decay': init_wd}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ddd2f9-3a0b-4388-8ae0-bb181d0dd2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraction = config_args['coreset_percent']\n",
    "\n",
    "opt_kwargs = {'lr': config_args['lr'], 'lr_decay': config_args['lr_decay'],\n",
    "                  'weight_decay': config_args['weight_decay'],\n",
    "                  'dampening': config_args['dampening'],\n",
    "                  'momentum': config_args['momentum'],\n",
    "                  'nesterov': config_args['nesterov']}\n",
    "\n",
    "optimizer = create_optimizer(\n",
    "        model_para, config_args['optimizer'], **opt_kwargs)\n",
    "scheduler = create_scheduler(optimizer, config_args, train_dir)\n",
    "early_stopping_scheduler = EarlyStopping(patience=config_args['early_patience'],\n",
    "                                         min_delta=config_args['early_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dcfde1c-3909-4386-8806-0cb4922ea33e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/20 01:21:58] __main__ INFO: hello\n",
      "[12/20 01:21:58] __main__ INFO: hello\n"
     ]
    }
   ],
   "source": [
    "#Results logging directory\n",
    "result_dname = 'results_gradmatch_{:.2f}_batch{}'.format(fraction, config_args['batch_size'])\n",
    "\n",
    "# if 'num_pipes' in config_args:\n",
    "#     result_dname += '_aug{}'.format(config_args['num_pipes'])\n",
    "    \n",
    "results_dir = osp.abspath(osp.expanduser(result_dname))\n",
    "logger = __get_logger(results_dir)\n",
    "logger.info(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99957f9-8b11-47d5-9629-beabb4a32856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import copy\n",
    "# copy.deepcopy(dss_args.model.loss)\n",
    "\n",
    "# # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe78fca-286c-4482-8ed3-f7ac8a2bb133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selection_strategy = 'GLISTER'\n",
    "# dss_args = dict(model=model,\n",
    "#                 loss=model.loss,\n",
    "#                 eta=0.01,\n",
    "#                 num_classes=1211,\n",
    "#                 num_epochs=30,\n",
    "#                 device='cuda',\n",
    "#                 fraction=0.25,\n",
    "#                 select_every=6,\n",
    "#                 kappa=0,\n",
    "#                 linear_layer=False,\n",
    "#                 selection_type='SL',\n",
    "#                 greedy='Stochastic')\n",
    "\n",
    "dss_args=dict(type=\"GradMatch\",\n",
    "                            fraction=fraction,\n",
    "                            select_every=6,\n",
    "                            lam=0.5,\n",
    "                            selection_type='PerBatch',\n",
    "                            v1=True,\n",
    "                            valid=False,\n",
    "                            kappa=0,\n",
    "                            eps=1e-100,\n",
    "                            linear_layer=False,\n",
    "                            model=model,\n",
    "                            loss=model.loss,\n",
    "                            eta = 0.001,\n",
    "                            num_classes = 1211,\n",
    "                            device = 'cuda'\n",
    "                            )\n",
    "\n",
    "dss_args = DotMap(dss_args)\n",
    "# dataloader = GLISTERDataLoader(train_loader, valid_loader, dss_args, logger, \n",
    "#                                   batch_size=config_args['batch_size'], \n",
    "#                                   shuffle=True,\n",
    "#                                   pin_memory=False)\n",
    "dataloader = GradMatchDataLoader(train_loader, valid_loader, dss_args, logger, \n",
    "                                  batch_size=config_args['batch_size'], \n",
    "                                  shuffle=True,\n",
    "                                  pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f77b6-3f6a-4f57-bf88-7bf18ac5b3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Arguments\n",
    "num_epochs = 30\n",
    "\n",
    "#Arguments for results logging\n",
    "print_every = 1\n",
    "# print_args = [\"val_loss\", \"val_acc\", \"tst_loss\", \"tst_acc\", \"time\"]\n",
    "print_args = [\"val_loss\", \"val_acc\", \"time\"]\n",
    "\n",
    "#Argumets for checkpointing\n",
    "save_every = 3\n",
    "is_save = True\n",
    "\n",
    "#Evaluation Metrics\n",
    "trn_losses = list()\n",
    "val_losses = list()\n",
    "tst_losses = list()\n",
    "subtrn_losses = list()\n",
    "timing = list()\n",
    "trn_acc = list()\n",
    "val_acc = list()  \n",
    "tst_acc = list()  \n",
    "subtrn_acc = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe76945-7e49-4aad-be74-c3e138c31acd",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7363f6cf-3c33-4997-ba00-e5d6e758e012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'augment_pipeline' in config_args:\n",
    "    num_pipes = config_args['num_pipes'] if 'num_pipes' in config_args else 1\n",
    "    augment_pipeline = []\n",
    "    for _, augment in enumerate(config_args['augment_pipeline']):\n",
    "        augment_pipeline.append(augment.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2216e7fd-3d51-4e4d-af8f-ba0d3653c780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m################################################# Training Loop #################################################\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m      5\u001b[0m     subtrn_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m     subtrn_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "################################################# Training Loop #################################################\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    subtrn_loss = 0\n",
    "    subtrn_correct = 0\n",
    "    subtrn_total = 0\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for _, (inputs, targets, weights) in tqdm(enumerate(dataloader), ncols=50):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        label = targets.to(device)\n",
    "        # targets = targets.to(device, non_blocking=True)\n",
    "        weights = weights.to(device)  \n",
    "        \n",
    "        if 'augment_pipeline' in config_args:\n",
    "            with torch.no_grad():\n",
    "                wavs_aug_tot = []\n",
    "                labels_aug_tot = []\n",
    "                weights_aug_tot = []\n",
    "                \n",
    "                wavs_aug_tot.append(inputs.cuda()) # data_shape [batch, 1,1,time]\n",
    "                labels_aug_tot.append(label.cuda())\n",
    "                weights_aug_tot.append(weights.cuda())\n",
    "                \n",
    "                wavs = inputs.squeeze().cuda()\n",
    "                wav_label = label.squeeze().cuda()\n",
    "                wav_weights = weights.squeeze().cuda()\n",
    "                \n",
    "                augs_idx = np.random.choice(len(augment_pipeline), size=num_pipes, replace=False)\n",
    "                augs_idx = set(augs_idx)\n",
    "                augs = [augment_pipeline[i] for i in augs_idx]\n",
    "                sample_idxs = [np.arange(len(wavs))] * len(augs_idx)\n",
    "\n",
    "                for data_idx, augment in zip(sample_idxs, augs):\n",
    "                    # Apply augment\n",
    "                    wavs_aug = augment(wavs[data_idx], torch.tensor([1.0]*len(wavs)).cuda())\n",
    "                    # Managing speed change\n",
    "                    if wavs_aug.shape[1] > wavs.shape[1]:\n",
    "                        wavs_aug = wavs_aug[:, 0 : wavs.shape[1]]\n",
    "                    else:\n",
    "                        zero_sig = torch.zeros_like(wavs)\n",
    "                        zero_sig[:, 0 : wavs_aug.shape[1]] = wavs_aug\n",
    "                        wavs_aug = zero_sig\n",
    "\n",
    "                    if 'concat_augment' in config_args and config_args['concat_augment']:\n",
    "                        wavs_aug_tot.append(wavs_aug.unsqueeze(1).unsqueeze(1))\n",
    "                        labels_aug_tot.append(wav_label[data_idx])\n",
    "                        weights_aug_tot.append(wav_weights[data_idx].cuda())\n",
    "                    else:\n",
    "                        wavs = wavs_aug\n",
    "                        wavs_aug_tot[0] = wavs_aug.unsqueeze(1).unsqueeze(1)\n",
    "                        labels_aug_tot[0] = wav_label[data_idx]\n",
    "                \n",
    "                inputs = torch.cat(wavs_aug_tot, dim=0)\n",
    "                label = torch.cat(labels_aug_tot)\n",
    "                weights = torch.cat(weights_aug_tot, dim=0)\n",
    "                \n",
    "        # print(inputs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, feats = model(inputs)\n",
    "        losses  = model.loss((outputs, feats), targets)\n",
    "        loss = torch.dot(losses, weights/(weights.sum()))\n",
    "        loss.backward()\n",
    "        \n",
    "        subtrn_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        _, predicted = outputs.max(1)\n",
    "        subtrn_total += targets.size(0)\n",
    "        subtrn_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    epoch_time = time.time() - start_time\n",
    "    timing.append(epoch_time)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ################################################# Evaluation Loop #################################################\n",
    "    \"\"\"\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        trn_loss, trn_correct, trn_total = 0, 0, 0\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        tst_correct, tst_total, tst_loss = 0, 0, 0\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        if (\"trn_loss\" in print_args) or (\"trn_acc\" in print_args):\n",
    "            with torch.no_grad():\n",
    "                for _, (inputs, targets) in enumerate(valid_loader):\n",
    "                    inputs, targets = inputs.to(device), \\\n",
    "                                      targets.to(device, non_blocking=True)\n",
    "                    \n",
    "                    outputs, feats = model(inputs)\n",
    "                    loss  = model.loss((outputs, feats), targets)\n",
    "                    \n",
    "                    trn_loss += loss.item()\n",
    "                    if \"trn_acc\" in print_args:\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        trn_total += targets.size(0)\n",
    "                        trn_correct += predicted.eq(targets).sum().item()\n",
    "                        \n",
    "                trn_losses.append(trn_loss)\n",
    "\n",
    "            if \"trn_acc\" in print_args:\n",
    "                trn_acc.append(trn_correct / trn_total)\n",
    "\n",
    "        if (\"val_loss\" in print_args) or (\"val_acc\" in print_args):\n",
    "            with torch.no_grad():\n",
    "                for _, (inputs, targets) in enumerate(valid_loader):\n",
    "                    inputs, targets = inputs.to(device), \\\n",
    "                                      targets.to(device, non_blocking=True)\n",
    "                    # outputs = model(inputs)\n",
    "                    # loss = criterion(outputs, targets)\n",
    "                    outputs, feats = model(inputs)\n",
    "                    loss  = model.loss((outputs, feats), targets)\n",
    "                    \n",
    "                    val_loss += loss.mean().item()\n",
    "                    if \"val_acc\" in print_args:\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            if \"val_acc\" in print_args:\n",
    "                val_acc.append(val_correct / val_total)\n",
    "\n",
    "        if (\"tst_loss\" in print_args) or (\"tst_acc\" in print_args):\n",
    "            with torch.no_grad():\n",
    "                for _, (inputs, targets) in enumerate(valid_loader):\n",
    "                    inputs, targets = inputs.to(device), \\\n",
    "                                      targets.to(device, non_blocking=True)\n",
    "                    \n",
    "                    # outputs = model(inputs)\n",
    "                    # loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    outputs, feats = model(inputs)\n",
    "                    loss  = model.loss((outputs, feats), targets)\n",
    "                    \n",
    "                    tst_loss += loss.mean().item()\n",
    "                    if \"tst_acc\" in print_args:\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                tst_losses.append(tst_loss)\n",
    "\n",
    "            if \"tst_acc\" in print_args:\n",
    "                tst_acc.append(tst_correct / tst_total)\n",
    "\n",
    "        if \"subtrn_acc\" in print_args:\n",
    "            subtrn_acc.append(subtrn_correct / subtrn_total)\n",
    "\n",
    "        if \"subtrn_losses\" in print_args:\n",
    "            subtrn_losses.append(subtrn_loss)\n",
    "\n",
    "        print_str = \"Epoch: \" + str(epoch + 1)\n",
    "\n",
    "        \"\"\"\n",
    "        ################################################# Results Printing #################################################\n",
    "        \"\"\"\n",
    "\n",
    "        for arg in print_args:\n",
    "\n",
    "            if arg == \"val_loss\":\n",
    "                print_str += \" , \" + \"Valid Loss: {:.8f}\".format(val_losses[-1])\n",
    "\n",
    "            if arg == \"val_acc\":\n",
    "                print_str += \" , \" + \"Valid Accuracy: {:.4f}\".format(val_acc[-1])\n",
    "\n",
    "            if arg == \"tst_loss\":\n",
    "                print_str += \" , \" + \"Test Loss: {:.8f}\".format(tst_losses[-1])\n",
    "\n",
    "            if arg == \"tst_acc\":\n",
    "                print_str += \" , \" + \"Test Accuracy: {:.4f}\".format(tst_acc[-1])\n",
    "\n",
    "            if arg == \"trn_loss\":\n",
    "                print_str += \" , \" + \"Train Loss: {:.8f}\".format(trn_losses[-1])\n",
    "\n",
    "            if arg == \"trn_acc\":\n",
    "                print_str += \" , \" + \"Train Accuracy: {:.4f}\".format(trn_acc[-1])\n",
    "\n",
    "            if arg == \"subtrn_loss\":\n",
    "                print_str += \" , \" + \"Subset Loss: {:.8f}\".format(subtrn_losses[-1])\n",
    "\n",
    "            if arg == \"subtrn_acc\":\n",
    "                print_str += \" , \" + \"Subset Accuracy: {:.4f}\".format(subtrn_acc[-1])\n",
    "\n",
    "            if arg == \"time\":\n",
    "                print_str += \" , \" + \"Timing: {:.2f}\".format(timing[-1])\n",
    "\n",
    "        logger.info(print_str)\n",
    "\n",
    "    \"\"\"\n",
    "    ################################################# Checkpoint Saving #################################################\n",
    "    \"\"\"\n",
    "\n",
    "    if ((epoch + 1) % save_every == 0) and is_save:\n",
    "\n",
    "        metric_dict = {}\n",
    "\n",
    "        for arg in print_args:\n",
    "            if arg == \"val_loss\":\n",
    "                metric_dict['val_loss'] = val_losses\n",
    "            if arg == \"val_acc\":\n",
    "                metric_dict['val_acc'] = val_acc\n",
    "            if arg == \"tst_loss\":\n",
    "                metric_dict['tst_loss'] = tst_losses\n",
    "            if arg == \"tst_acc\":\n",
    "                metric_dict['tst_acc'] = tst_acc\n",
    "            if arg == \"trn_loss\":\n",
    "                metric_dict['trn_loss'] = trn_losses\n",
    "            if arg == \"trn_acc\":\n",
    "                metric_dict['trn_acc'] = trn_acc\n",
    "            if arg == \"subtrn_loss\":\n",
    "                metric_dict['subtrn_loss'] = subtrn_losses\n",
    "            if arg == \"subtrn_acc\":\n",
    "                metric_dict['subtrn_acc'] = subtrn_acc\n",
    "            if arg == \"time\":\n",
    "                metric_dict['time'] = timing\n",
    "\n",
    "        ckpt_state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'loss': model.loss,\n",
    "            'metrics': metric_dict\n",
    "        }\n",
    "\n",
    "        # save checkpoint\n",
    "        save_ckpt(ckpt_state, results_dir + '/model.pt')\n",
    "        logger.info(\"Model checkpoint saved at epoch: {0:d}\".format(epoch + 1))\n",
    "\n",
    "\"\"\"\n",
    "################################################# Results Summary #################################################\n",
    "\"\"\"\n",
    "\n",
    "logger.info(\"{0:s} Selection Run---------------------------------\".format(selection_strategy))\n",
    "logger.info(\"Final SubsetTrn: {0:f}\".format(subtrn_loss))\n",
    "if \"val_loss\" in print_args:\n",
    "    if \"val_acc\" in print_args:\n",
    "        logger.info(\"Valid Loss: %.2f , Validation Accuracy: %.2f\", val_loss, val_acc[-1])\n",
    "    else:\n",
    "        logger.info(\"Valid Loss: %.2f\", val_loss)\n",
    "\n",
    "if \"tst_loss\" in print_args:\n",
    "    if \"tst_acc\" in print_args:\n",
    "        logger.info(\"Test Loss: %.2f, Test Accuracy: %.2f\", tst_loss, tst_acc[-1])\n",
    "    else:\n",
    "        logger.info(\"Test Data Loss: %f\", tst_loss)\n",
    "        \n",
    "logger.info('---------------------------------------------------------------------')\n",
    "logger.info(selection_strategy)\n",
    "logger.info('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532d25d-5e5e-41d9-90cd-a3cc650484c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "################################################# Final Results Logging #################################################\n",
    "\"\"\"\n",
    "\n",
    "if \"val_acc\" in print_args:\n",
    "    val_str = \"Valid Accuracy, \" + \" , \".join([str(val) for val in val_acc])\n",
    "    logger.info(val_str)\n",
    "\n",
    "if \"tst_acc\" in print_args:\n",
    "    tst_str = \"Test Accuracy, \" + \" , \".join([str(tst) for tst in tst_acc])\n",
    "    logger.info(tst_str)\n",
    "\n",
    "if \"time\" in print_args:\n",
    "    time_str = \"Time, \" + \" , \".join([str(t) for t in timing])\n",
    "    logger.info(timing)\n",
    "\n",
    "timing_array = np.array(timing)\n",
    "cum_timing = list(generate_cumulative_timing(timing_array))\n",
    "logger.info(\"Total time taken by %s = %.4f \", selection_strategy, cum_timing[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d9a92-1b5e-429b-97a5-5fa7aa9f5ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cords",
   "language": "python",
   "name": "cords"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
