{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import autograd\n",
    "import higher\n",
    "import itertools\n",
    "import os\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "torch.distributed.init_process_group(backend=\"nccl\", init_method='tcp://localhost:32458', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network (LeNet-5)  \n",
    "# from https://github.com/bollakarthikeya/LeNet-5-PyTorch/blob/master/lenet5_gpu.py\n",
    "class LeNet5(torch.nn.Module):          \n",
    "    def __init__(self):     \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2) \n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   \n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.max_pool_1(x) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool_2(x)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_n_accuracy(model, criterion, data_loader, args, num_classes=2):\n",
    "    \"\"\" Returns the loss and total accuracy, per class accuracy on the supplied data loader \"\"\"\n",
    "    \n",
    "    criterion.reduction = 'mean'\n",
    "    model.eval()                                     \n",
    "    total_loss, correctly_labeled_samples = 0, 0\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    \n",
    "    # forward-pass to get loss and predictions of the current batch\n",
    "    for _, (inputs, labels) in enumerate(data_loader):\n",
    "#         inputs, labels = inputs.to(device=args['device'], non_blocking=True),\\\n",
    "#                 labels.to(device=args['device'], non_blocking=True)\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # compute the total loss over minibatch\n",
    "        outputs = model(inputs)\n",
    "        avg_minibatch_loss = criterion(outputs, labels.type_as(outputs))\n",
    "        total_loss += avg_minibatch_loss.item()*outputs.shape[0]\n",
    "                        \n",
    "        # get num of correctly predicted inputs in the current batch\n",
    "        pred_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        correctly_labeled_samples += torch.sum(torch.eq(pred_labels.view(-1), labels)).item()\n",
    "        # fill confusion_matrix\n",
    "        for t, p in zip(labels.view(-1), pred_labels.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "                                \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = correctly_labeled_samples / len(data_loader.dataset)\n",
    "    per_class_accuracy = confusion_matrix.diag() / confusion_matrix.sum(1)\n",
    "    return avg_loss, (accuracy, per_class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imbalanced_datasets(train_dataset, test_dataset, imbalance=0.995, train_size=5000, meta_size=10):\n",
    "    # returns an imbalanced mnist dataset of 9 and 4s where imbalance favors 9s\n",
    "    \n",
    "    # a balanced test dataset\n",
    "    test_9_idxs = test_dataset.targets == 9\n",
    "    test_4_idxs = test_dataset.targets == 4\n",
    "    test_9_data = test_dataset.data[test_9_idxs][:982]\n",
    "    test_4_data = test_dataset.data[test_4_idxs][:982] # num of 4 samples \n",
    "    test_data = torch.cat((test_9_data, test_4_data))\n",
    "    test_targets = torch.cat( (torch.ones(len(test_9_data))*1, torch.ones(len(test_4_data))*0 ) )\n",
    "    test_dataset.data = test_data\n",
    "    test_dataset.targets = test_targets\n",
    "    \n",
    "    # imbalanced training dataset\n",
    "    n_9s = int(train_size * imbalance) # 0.995*5000 of samples are 9s\n",
    "    n_4s = train_size - n_9s           # 0.005*5000 of samples are 4s\n",
    "    train_9_idxs = train_dataset.targets == 9\n",
    "    train_4_idxs = train_dataset.targets == 4 \n",
    "    train_9_data = train_dataset.data[train_9_idxs][:n_9s]\n",
    "    train_4_data = train_dataset.data[train_4_idxs][:n_4s]\n",
    "    train_data = torch.cat((train_9_data, train_4_data))\n",
    "    train_targets = torch.cat( (torch.ones(len(train_9_data))*1, torch.ones(len(train_4_data))*0 ) )\n",
    "    train_dataset.data = train_data\n",
    "    train_dataset.targets = train_targets\n",
    "    \n",
    "    # a balanced meta dataset for weighting samples (which is subset of training dataset)\n",
    "    # note that we have relabed 9s as 1 and 4s as 0\n",
    "    meta_dataset = copy.deepcopy(train_dataset)\n",
    "    meta_9_idxs = meta_dataset.targets == 1\n",
    "    meta_4_idxs = meta_dataset.targets == 0\n",
    "    meta_9_data = meta_dataset.data[meta_9_idxs][:(meta_size // 2)] # meta-size = 5\n",
    "    meta_4_data = meta_dataset.data[meta_4_idxs][:(meta_size // 2)]\n",
    "    meta_data = torch.cat((meta_9_data, meta_4_data))\n",
    "    meta_dataset.data = meta_data\n",
    "    meta_targets = torch.cat( (torch.ones(len(meta_9_data))*1, torch.ones(len(meta_4_data))*0 ) )\n",
    "    meta_dataset.targets = meta_targets\n",
    "    \n",
    "    return train_dataset, meta_dataset, test_dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'bs':100, 'lr':1e-3, 'n_epochs':150, 'device':'cuda:1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "train_dataset, meta_dataset, test_dataset = get_imbalanced_datasets(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=args['bs'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=args['bs'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "meta_loader = DataLoader(meta_dataset, batch_size=args['bs'], shuffle=True, pin_memory=True)\n",
    "meta_loader = itertools.cycle(meta_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangwenhao/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/distributed.py:364: UserWarning: Single-Process Multi-GPU is not the recommended mode for DDP. In this mode, each DDP instance operates on multiple devices and creates multiple module replicas within one process. The overhead of scatter/gather and GIL contention in every forward pass can slow down training. Please consider using one DDP instance per device or per module replica by explicitly setting device_ids or CUDA_VISIBLE_DEVICES. \n",
      "  \"Single-Process Multi-GPU is not the recommended mode for \"\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()#.to(args['device'])\n",
    "model = DistributedDataParallel(model.cuda())\n",
    "opt = optim.SGD(model.parameters(), lr=args['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss().to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, end_time = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "writer = SummaryWriter('logs/baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Train/Test Loss: 0.030 / 2.578, Train/Test Acc: 0.995 / 0.500: 100%|██████████| 150/150 [06:53<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 413.97 seconds / 6.90 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time.record()\n",
    "pbar = tqdm(range(1, args['n_epochs']+1))\n",
    "for ep in pbar:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "#         inputs, labels = inputs.to(device=args['device'], non_blocking=True),\\\n",
    "#                             labels.to(device=args['device'], non_blocking=True)\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        minibatch_loss = criterion(outputs, labels.type_as(outputs))\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keep track of epoch loss/accuracy\n",
    "        train_loss += minibatch_loss.item()*outputs.shape[0]\n",
    "        pred_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        \n",
    "    # inference after epoch\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = train_loss/len(train_dataset), train_acc/len(train_dataset)       \n",
    "        test_loss, (test_acc, test_per_class_acc) = get_loss_n_accuracy(model, criterion, test_loader, args)                                  \n",
    "        # log/print data\n",
    "        writer.add_scalar('Test/Loss', test_loss, ep)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, ep)\n",
    "        writer.add_scalar('Training/Loss', train_loss, ep)\n",
    "        writer.add_scalar('Training/Accuracy', train_acc, ep)\n",
    "        pbar.set_description(' Train/Test Loss: {:.3f} / {:.3f}, Train/Test Acc: {:.3f} / {:.3f}'.format(train_loss,test_loss,train_acc,test_acc))  \n",
    "\n",
    "end_time.record()\n",
    "torch.cuda.synchronize()\n",
    "time_elapsed_secs = start_time.elapsed_time(end_time)/10**3\n",
    "time_elapsed_mins = time_elapsed_secs/60\n",
    "print(f'Training took {time_elapsed_secs:.2f} seconds / {time_elapsed_mins:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5().to(args['device'])\n",
    "opt = optim.SGD(model.parameters(), lr=args['lr'])\n",
    "start_time, end_time = torch.cuda.Event(enable_timing=True),\\\n",
    "                        torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "writer = SummaryWriter('logs/weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Train/Test Loss: 0.005 / 0.159, Train/Test Acc: 0.978 / 0.943: 100%|██████████| 150/150 [01:41<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 101.50 seconds / 1.69 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time.record()\n",
    "pbar = tqdm(range(1, args['n_epochs']+1))\n",
    "\n",
    "for ep in pbar:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device=args['device'], non_blocking=True),\\\n",
    "                            labels.to(device=args['device'], non_blocking=True)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        with higher.innerloop_ctx(model, opt) as (meta_model, meta_opt):\n",
    "            # 1. Update meta model on training data\n",
    "            meta_train_outputs = meta_model(inputs)\n",
    "            criterion.reduction = 'none'\n",
    "            meta_train_loss = criterion(meta_train_outputs, labels.type_as(outputs))\n",
    "            eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=args['device'])\n",
    "            meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "            meta_opt.step(meta_train_loss)\n",
    "\n",
    "            # 2. Compute grads of eps on meta validation data\n",
    "            meta_inputs, meta_labels =  next(meta_loader)\n",
    "            meta_inputs, meta_labels = meta_inputs.to(device=args['device'], non_blocking=True),\\\n",
    "                             meta_labels.to(device=args['device'], non_blocking=True)\n",
    "\n",
    "            meta_val_outputs = meta_model(meta_inputs)\n",
    "            criterion.reduction = 'mean'\n",
    "            meta_val_loss = criterion(meta_val_outputs, meta_labels.type_as(outputs))\n",
    "            eps_grads = torch.autograd.grad(meta_val_loss, eps)[0].detach()\n",
    "#             print(meta_val_loss)\n",
    "#             print(eps)\n",
    "#             break\n",
    "\n",
    "        # 3. Compute weights for current training batch\n",
    "        w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "        l1_norm = torch.sum(w_tilde)\n",
    "        if l1_norm != 0:\n",
    "            w = w_tilde / l1_norm\n",
    "        else:\n",
    "            w = w_tilde\n",
    "\n",
    "        # 4. Train model on weighted batch\n",
    "        outputs = model(inputs)\n",
    "        criterion.reduction = 'none'\n",
    "        minibatch_loss = criterion(outputs, labels.type_as(outputs))\n",
    "        minibatch_loss = torch.sum(w * minibatch_loss)\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keep track of epoch loss/accuracy\n",
    "        train_loss += minibatch_loss.item()*outputs.shape[0]\n",
    "        pred_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "\n",
    "    # inference after epoch\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = train_loss/len(train_dataset), train_acc/len(train_dataset)       \n",
    "        test_loss, (test_acc, test_per_class_acc) = get_loss_n_accuracy(model, criterion, test_loader, args)                                  \n",
    "        # log/print data\n",
    "        writer.add_scalar('Test/Loss', test_loss, ep)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, ep)\n",
    "        writer.add_scalar('Training/Loss', train_loss, ep)\n",
    "        writer.add_scalar('Training/Accuracy', train_acc, ep)\n",
    "        pbar.set_description(' Train/Test Loss: {:.3f} / {:.3f}, Train/Test Acc: {:.3f} / {:.3f}'.format(train_loss,test_loss,train_acc,test_acc))  \n",
    "   \n",
    "        \n",
    "end_time.record()\n",
    "torch.cuda.synchronize()\n",
    "time_elapsed_secs = start_time.elapsed_time(end_time)/10**3\n",
    "time_elapsed_mins = time_elapsed_secs/60\n",
    "print(f'Training took {time_elapsed_secs:.2f} seconds / {time_elapsed_mins:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-97b47f6a4321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmeta_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_val_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0meps_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m#             print(meta_val_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#             print(eps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    190\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    191\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "start_time.record()\n",
    "pbar = tqdm(range(1, args['n_epochs']+1))\n",
    "\n",
    "for ep in pbar:\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device=args['device'], non_blocking=True),\\\n",
    "                            labels.to(device=args['device'], non_blocking=True)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        meta_model = LeNet5()\n",
    "        meta_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            meta_model.to(device=args['device'])\n",
    "            \n",
    "        meta_opt = optim.SGD(meta_model.parameters(), lr=args['lr'])\n",
    "        # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n",
    "        meta_train_outputs = meta_model(inputs)\n",
    "        criterion.reduction = 'none'\n",
    "        meta_train_loss = criterion(meta_train_outputs, labels.type_as(outputs))\n",
    "        eps = torch.zeros(meta_train_loss.size(), requires_grad=True, device=args['device'])\n",
    "        meta_train_loss = torch.sum(eps * meta_train_loss)\n",
    "        meta_model.zero_grad()\n",
    "#         meta_opt.step(meta_train_loss)\n",
    "        \n",
    "        # Line 6 perform a parameter update\n",
    "        grads = torch.autograd.grad(meta_train_loss, (meta_model.parameters()), create_graph=True)\n",
    "        meta_opt.step()\n",
    "#         meta_model.update_params(hyperparameters['lr'], source_params=grads)\n",
    "        \n",
    "        # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n",
    "#         y_g_hat = meta_net(val_data)\n",
    "#         l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat,val_labels)\n",
    "#         grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n",
    "        \n",
    "        meta_inputs, meta_labels =  next(meta_loader)\n",
    "        meta_inputs, meta_labels = meta_inputs.to(device=args['device'], non_blocking=True),\\\n",
    "                         meta_labels.to(device=args['device'], non_blocking=True)\n",
    "\n",
    "        meta_val_outputs = meta_model(meta_inputs)\n",
    "        criterion.reduction = 'mean'\n",
    "        meta_val_loss = criterion(meta_val_outputs, meta_labels.type_as(outputs))\n",
    "        eps_grads = torch.autograd.grad(meta_val_loss, eps)[0].detach()\n",
    "#             print(meta_val_loss)\n",
    "#             print(eps)\n",
    "#             break\n",
    "\n",
    "        # 3. Compute weights for current training batch\n",
    "        w_tilde = torch.clamp(-eps_grads, min=0)\n",
    "        l1_norm = torch.sum(w_tilde)\n",
    "        if l1_norm != 0:\n",
    "            w = w_tilde / l1_norm\n",
    "        else:\n",
    "            w = w_tilde\n",
    "\n",
    "        # 4. Train model on weighted batch\n",
    "        outputs = model(inputs)\n",
    "        criterion.reduction = 'none'\n",
    "        minibatch_loss = criterion(outputs, labels.type_as(outputs))\n",
    "        minibatch_loss = torch.sum(w * minibatch_loss)\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # keep track of epoch loss/accuracy\n",
    "        train_loss += minibatch_loss.item()*outputs.shape[0]\n",
    "        pred_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "\n",
    "    # inference after epoch\n",
    "    with torch.no_grad():\n",
    "        train_loss, train_acc = train_loss/len(train_dataset), train_acc/len(train_dataset)       \n",
    "        test_loss, (test_acc, test_per_class_acc) = get_loss_n_accuracy(model, criterion, test_loader, args)                                  \n",
    "        # log/print data\n",
    "        writer.add_scalar('Test/Loss', test_loss, ep)\n",
    "        writer.add_scalar('Test/Accuracy', test_acc, ep)\n",
    "        writer.add_scalar('Training/Loss', train_loss, ep)\n",
    "        writer.add_scalar('Training/Accuracy', train_acc, ep)\n",
    "        pbar.set_description(' Train/Test Loss: {:.3f} / {:.3f}, Train/Test Acc: {:.3f} / {:.3f}'.format(train_loss,test_loss,train_acc,test_acc))  \n",
    "   \n",
    "        \n",
    "end_time.record()\n",
    "torch.cuda.synchronize()\n",
    "time_elapsed_secs = start_time.elapsed_time(end_time)/10**3\n",
    "time_elapsed_mins = time_elapsed_secs/60\n",
    "print(f'Training took {time_elapsed_secs:.2f} seconds / {time_elapsed_mins:.2f} minutes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
