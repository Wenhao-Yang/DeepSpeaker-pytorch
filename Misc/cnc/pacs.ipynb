{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482944b0-73ea-4711-b327-3b993749fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d5cec8-02a3-4bce-a848-0fcbe3d09421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/pacs-train loaded successfully.\n",
      "\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-train\n",
      "\n",
      " \r"
     ]
    }
   ],
   "source": [
    "ds = deeplake.load(\"hub://activeloop/pacs-train\")\n",
    "ds_val = deeplake.load(\"hub://activeloop/pacs-val\")\n",
    "ds_test = deeplake.load(\"hub://activeloop/pacs-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7798ab1-656f-4e94-a56c-ceecaa7a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ds.pytorch(num_workers=0, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d923edb-5e48-4a01-8554-1656dcc166c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HINT: Please forward the port - 59017 to your local machine, if you are running on the cloud.\n",
      " * Serving Flask app 'dataset_visualizer'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"800\"\n",
       "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://activeloop/pacs-train&token=eyJhbGciOiJIUzUxMiIsImlhdCI6MTY3OTQ4OTE4MiwiZXhwIjoxNjgzMDg5MTgyfQ.eyJpZCI6InB1YmxpYyJ9.Uicuc6DtoaJikxP-F_ctpNH3R4GquIZnlUTFfRp4Ax_pKMntsjA7ZHFwzS2WhW0D_gxZTAV6rEKR4g3Er2sHNQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7658a182e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171423d7-1d4d-4c7b-9cd7-197d07a7d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTForMaskedImageModeling\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e1d340-a505-4362-b4c5-416d12d14bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859d4598134a4a8aadc8684bb26d390c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2127b3dd91ec42439f0337df715fec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b37ff920664a4c912eec938eaf97dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForMaskedImageModeling: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForMaskedImageModeling from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForMaskedImageModeling from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForMaskedImageModeling were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['embeddings.mask_token', 'decoder.0.bias', 'decoder.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'reconstruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_144264/355944561.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_pixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_pixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'reconstruction'"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTForMaskedImageModeling.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "num_patches = (model.config.image_size // model.config.patch_size) ** 2\n",
    "pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "# create random boolean mask of shape (batch_size, num_patches)\n",
    "bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n",
    "\n",
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction\n",
    "list(reconstructed_pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799b8eb-3eb2-4abc-97be-93ffe356f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
